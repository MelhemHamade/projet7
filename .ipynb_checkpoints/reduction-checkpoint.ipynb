{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182a2d58-dc58-4b62-9d0b-59766de1b65c",
   "metadata": {},
   "source": [
    " # Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b26085f9-483f-4763-b507-84b0bb1785dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train_merged.csv')\n",
    "test = pd.read_csv( 'test_merged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27646627-880f-4fe3-9799-0de5e471f79b",
   "metadata": {},
   "source": [
    "# Optimiser par corrélation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a054b9-9319-445a-a3f4-28bddef35534",
   "metadata": {},
   "source": [
    "## Coorrélation des variables numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ac4290-047a-4c3a-b752-c2b9a1805f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def optimize_dataframe_by_correlation(df, threshold):\n",
    "    \"\"\"\n",
    "    Optimise un DataFrame en supprimant les variables redondantes basées sur un seuil de corrélation.\n",
    "\n",
    "    :param df: DataFrame à optimiser.\n",
    "    :param threshold: Seuil de corrélation pour identifier les paires de variables redondantes.\n",
    "    :return: DataFrame optimisé avec les variables redondantes supprimées.\n",
    "    \"\"\"\n",
    "    # Sélectionner uniquement les variables numériques et calculer la matrice de corrélation\n",
    "    num_vars = df.select_dtypes(include=[np.number])\n",
    "    num_corr_matrix = num_vars.corr()\n",
    "\n",
    "    # Trouver les paires de variables hautement corrélées\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(num_corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(num_corr_matrix.iloc[i, j]) > threshold:\n",
    "                high_corr_pairs.append((num_corr_matrix.columns[i], num_corr_matrix.columns[j], num_corr_matrix.iloc[i, j]))\n",
    "\n",
    "    # Identifier les variables à supprimer\n",
    "    to_remove = set()\n",
    "    for var1, var2, _ in high_corr_pairs:\n",
    "        if var1 not in to_remove and var2 not in to_remove:\n",
    "            if df[var1].isna().sum() > df[var2].isna().sum():\n",
    "                to_remove.add(var1)\n",
    "            else:\n",
    "                to_remove.add(var2)\n",
    "\n",
    "    # Supprimer les variables redondantes\n",
    "    df_reduced = df.drop(columns=to_remove)\n",
    "\n",
    "    return df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "963cf228-b046-4f10-a246-7d42944f2ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 125)\n"
     ]
    }
   ],
   "source": [
    "# Appliquer la méthode au DataFrame\n",
    "threshold = 0.7 \n",
    "train_optimized_num = optimize_dataframe_by_correlation(train, threshold)\n",
    "print(train_optimized_num.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b32c73-eaa9-4813-b1bd-1a7e2bbf1490",
   "metadata": {},
   "source": [
    "## Coorrélation des variables catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60402b8a-4c5b-4172-ac5c-2a784f23ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def optimize_categorical_dataframe(df, threshold_p_value):\n",
    "    \"\"\"\n",
    "    Optimise un DataFrame en supprimant les variables catégorielles redondantes basées sur un seuil de p-value.\n",
    "\n",
    "    :param df: DataFrame à optimiser.\n",
    "    :param threshold_p_value: Seuil de p-value pour identifier les paires de variables catégorielles associées.\n",
    "    :return: DataFrame optimisé avec les variables catégorielles redondantes supprimées.\n",
    "    \"\"\"\n",
    "    # Sélectionner uniquement les variables catégorielles\n",
    "    cat_vars = df.select_dtypes(exclude=[np.number])\n",
    "    cat_var_names = cat_vars.columns.tolist()\n",
    "\n",
    "    # Trouver les paires de variables catégorielles associées\n",
    "    associated_pairs = []\n",
    "    for i in range(len(cat_var_names)):\n",
    "        for j in range(i):\n",
    "            contingency_table = pd.crosstab(df[cat_var_names[i]], df[cat_var_names[j]])\n",
    "            chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "            if p < threshold_p_value:\n",
    "                associated_pairs.append((cat_var_names[i], cat_var_names[j], p))\n",
    "\n",
    "    # Identifier les variables catégorielles à supprimer\n",
    "    to_remove = set()\n",
    "    for var1, var2, p_value in associated_pairs:\n",
    "        if p_value < threshold_p_value:\n",
    "            if df[var1].isna().sum() > df[var2].isna().sum():\n",
    "                to_remove.add(var1)\n",
    "            else:\n",
    "                to_remove.add(var2)\n",
    "\n",
    "    # Supprimer les variables catégorielles redondantes\n",
    "    df_reduced = df.drop(columns=to_remove)\n",
    "\n",
    "    return df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a8fe7c-5421-4134-889f-f9ee9853e7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 109)\n"
     ]
    }
   ],
   "source": [
    "# Appliquer la méthode au DataFrame\n",
    "threshold_p_value = 0.05  \n",
    "train_optimized_num_cat = optimize_categorical_dataframe(train_optimized_num, threshold_p_value)\n",
    "print(train_optimized_num_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0373a65c-099c-4155-ab13-636affeecf7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bf1ccd3-233c-4b59-8428-c3d43b7c1ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def presence_rate(data):\n",
    "    # Calculer le pourcentage de présence des données pour chaque colonne\n",
    "    presence_rate = (1 - data.isnull().mean()) * 100\n",
    "    \n",
    "    # Trier la série presence_rate par valeurs décroissantes\n",
    "    sorted_presence_rate = presence_rate.sort_values(ascending=False)\n",
    "    \n",
    "    # Formater chaque pourcentage avec deux chiffres après la virgule et ajouter le caractère '%'\n",
    "    formatted_presence_rate = sorted_presence_rate.apply(lambda x: f'{x:.2f} %')\n",
    "    \n",
    "    # Retourner le taux de présence formaté et trié\n",
    "    return formatted_presence_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aaaccfd-5e5d-4e1e-9580-315e75f73c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 109)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_optimized_num_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb2abaa3-9151-4c0e-9f10-14d2fd1e6d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_PUBLISH</th>\n",
       "      <th>OWN_CAR_AGE</th>\n",
       "      <th>FLAG_MOBIL</th>\n",
       "      <th>FLAG_EMP_PHONE</th>\n",
       "      <th>FLAG_WORK_PHONE</th>\n",
       "      <th>FLAG_CONT_MOBILE</th>\n",
       "      <th>FLAG_PHONE</th>\n",
       "      <th>FLAG_EMAIL</th>\n",
       "      <th>REGION_RATING_CLIENT_W_CITY</th>\n",
       "      <th>HOUR_APPR_PROCESS_START</th>\n",
       "      <th>REG_REGION_NOT_LIVE_REGION</th>\n",
       "      <th>LIVE_REGION_NOT_WORK_REGION</th>\n",
       "      <th>REG_CITY_NOT_LIVE_CITY</th>\n",
       "      <th>LIVE_CITY_NOT_WORK_CITY</th>\n",
       "      <th>ORGANIZATION_TYPE</th>\n",
       "      <th>EXT_SOURCE_1</th>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "      <th>BASEMENTAREA_MEDI</th>\n",
       "      <th>YEARS_BEGINEXPLUATATION_MEDI</th>\n",
       "      <th>YEARS_BUILD_MEDI</th>\n",
       "      <th>COMMONAREA_MEDI</th>\n",
       "      <th>ENTRANCES_MEDI</th>\n",
       "      <th>FLOORSMAX_MEDI</th>\n",
       "      <th>LANDAREA_MEDI</th>\n",
       "      <th>NONLIVINGAPARTMENTS_MEDI</th>\n",
       "      <th>NONLIVINGAREA_MEDI</th>\n",
       "      <th>TOTALAREA_MODE</th>\n",
       "      <th>OBS_60_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DEF_60_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DAYS_LAST_PHONE_CHANGE</th>\n",
       "      <th>FLAG_DOCUMENT_2</th>\n",
       "      <th>FLAG_DOCUMENT_3</th>\n",
       "      <th>FLAG_DOCUMENT_4</th>\n",
       "      <th>FLAG_DOCUMENT_5</th>\n",
       "      <th>FLAG_DOCUMENT_6</th>\n",
       "      <th>FLAG_DOCUMENT_7</th>\n",
       "      <th>FLAG_DOCUMENT_8</th>\n",
       "      <th>FLAG_DOCUMENT_9</th>\n",
       "      <th>FLAG_DOCUMENT_10</th>\n",
       "      <th>FLAG_DOCUMENT_11</th>\n",
       "      <th>FLAG_DOCUMENT_12</th>\n",
       "      <th>FLAG_DOCUMENT_13</th>\n",
       "      <th>FLAG_DOCUMENT_14</th>\n",
       "      <th>FLAG_DOCUMENT_15</th>\n",
       "      <th>FLAG_DOCUMENT_16</th>\n",
       "      <th>FLAG_DOCUMENT_17</th>\n",
       "      <th>FLAG_DOCUMENT_18</th>\n",
       "      <th>FLAG_DOCUMENT_19</th>\n",
       "      <th>FLAG_DOCUMENT_20</th>\n",
       "      <th>FLAG_DOCUMENT_21</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "      <th>BUREAU_DAYS_CREDIT_max</th>\n",
       "      <th>BUREAU_CREDIT_DAY_OVERDUE_sum</th>\n",
       "      <th>BUREAU_DAYS_CREDIT_ENDDATE_max</th>\n",
       "      <th>BUREAU_AMT_CREDIT_MAX_OVERDUE_max</th>\n",
       "      <th>BUREAU_CNT_CREDIT_PROLONG_sum</th>\n",
       "      <th>BUREAU_AMT_CREDIT_SUM_sum</th>\n",
       "      <th>BUREAU_AMT_CREDIT_SUM_DEBT_sum</th>\n",
       "      <th>BUREAU_AMT_CREDIT_SUM_OVERDUE_mean</th>\n",
       "      <th>BUREAU_DAYS_CREDIT_UPDATE_mean</th>\n",
       "      <th>BUREAU_AMT_ANNUITY_sum</th>\n",
       "      <th>ca_Active</th>\n",
       "      <th>ca_Bad debt</th>\n",
       "      <th>ca_Sold</th>\n",
       "      <th>ct_Another type of loan</th>\n",
       "      <th>ct_Car loan</th>\n",
       "      <th>ct_Cash loan (non-earmarked)</th>\n",
       "      <th>ct_Consumer credit</th>\n",
       "      <th>ct_Credit card</th>\n",
       "      <th>ct_Interbank credit</th>\n",
       "      <th>ct_Loan for business development</th>\n",
       "      <th>ct_Loan for purchase of shares (margin lending)</th>\n",
       "      <th>ct_Loan for the purchase of equipment</th>\n",
       "      <th>ct_Loan for working capital replenishment</th>\n",
       "      <th>ct_Microloan</th>\n",
       "      <th>ct_Mobile operator loan</th>\n",
       "      <th>ct_Mortgage</th>\n",
       "      <th>ct_Real estate loan</th>\n",
       "      <th>ct_Unknown type of loan</th>\n",
       "      <th>MONTHS_BALANCE_max</th>\n",
       "      <th>CNT_INSTALMENT_FUTURE_sum</th>\n",
       "      <th>SK_DPD_sum_x</th>\n",
       "      <th>SK_DPD_DEF_sum_x</th>\n",
       "      <th>AMT_CREDIT_LIMIT_ACTUAL_sum</th>\n",
       "      <th>AMT_PAYMENT_CURRENT_sum</th>\n",
       "      <th>CNT_DRAWINGS_CURRENT_sum</th>\n",
       "      <th>SK_DPD_sum_y</th>\n",
       "      <th>SK_DPD_DEF_sum_y</th>\n",
       "      <th>AMT_DOWN_PAYMENT_sum</th>\n",
       "      <th>RATE_DOWN_PAYMENT_mean</th>\n",
       "      <th>CNT_PAYMENT_sum</th>\n",
       "      <th>INST_NUM_INSTALMENT_VERSION_mean</th>\n",
       "      <th>INST_NUM_INSTALMENT_NUMBER_max</th>\n",
       "      <th>INST_DAYS_INSTALMENT_mean</th>\n",
       "      <th>INST_AMT_PAYMENT_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>0.018801</td>\n",
       "      <td>-9461</td>\n",
       "      <td>-3648.0</td>\n",
       "      <td>-2120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>0.083037</td>\n",
       "      <td>0.262949</td>\n",
       "      <td>0.139376</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.6243</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>0.0690</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1134.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>780.0</td>\n",
       "      <td>5043.645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>865055.565</td>\n",
       "      <td>245781.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-499.875000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.052632</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-295.000000</td>\n",
       "      <td>219625.695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>-16765</td>\n",
       "      <td>-1186.0</td>\n",
       "      <td>-291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>School</td>\n",
       "      <td>0.311267</td>\n",
       "      <td>0.622246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.7987</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.2917</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-828.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-606.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1216.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017400.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-816.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6885.00</td>\n",
       "      <td>0.050030</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-1378.160000</td>\n",
       "      <td>1618864.650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>-19046</td>\n",
       "      <td>-4260.0</td>\n",
       "      <td>-2531</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Government</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.555912</td>\n",
       "      <td>0.729567</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-815.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-408.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-382.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>189037.800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-532.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4860.00</td>\n",
       "      <td>0.212008</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-754.000000</td>\n",
       "      <td>21288.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>-19005</td>\n",
       "      <td>-9833.0</td>\n",
       "      <td>-2437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.650442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-617.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1620000.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69680.34</td>\n",
       "      <td>0.163412</td>\n",
       "      <td>138.0</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-252.250000</td>\n",
       "      <td>1007153.415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>-19932</td>\n",
       "      <td>-4311.0</td>\n",
       "      <td>-3458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Religion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.322738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1106.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1149.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-783.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>146250.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-783.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>592.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6781.50</td>\n",
       "      <td>0.159516</td>\n",
       "      <td>124.0</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-1028.606061</td>\n",
       "      <td>806127.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99000.0</td>\n",
       "      <td>490495.5</td>\n",
       "      <td>0.035792</td>\n",
       "      <td>-16941</td>\n",
       "      <td>-4970.0</td>\n",
       "      <td>-477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.354225</td>\n",
       "      <td>0.621226</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2536.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>471.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468445.500</td>\n",
       "      <td>240057.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-611.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>28142.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16645.50</td>\n",
       "      <td>0.073051</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.028571</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1263.914286</td>\n",
       "      <td>957617.595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100009</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>171000.0</td>\n",
       "      <td>1560726.0</td>\n",
       "      <td>0.035792</td>\n",
       "      <td>-13778</td>\n",
       "      <td>-1213.0</td>\n",
       "      <td>-619</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>0.774761</td>\n",
       "      <td>0.724000</td>\n",
       "      <td>0.492060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1562.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-239.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1402.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4800811.500</td>\n",
       "      <td>1077349.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-851.611111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64422.00</td>\n",
       "      <td>0.126602</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-855.823529</td>\n",
       "      <td>487995.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>1530000.0</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>-18850</td>\n",
       "      <td>-4597.0</td>\n",
       "      <td>-2379</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.714279</td>\n",
       "      <td>0.540654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1070.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1138.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>990000.000</td>\n",
       "      <td>348007.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-578.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-904.000000</td>\n",
       "      <td>274492.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>1019610.0</td>\n",
       "      <td>0.018634</td>\n",
       "      <td>-20099</td>\n",
       "      <td>-7427.0</td>\n",
       "      <td>-3514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>XNA</td>\n",
       "      <td>0.587334</td>\n",
       "      <td>0.205747</td>\n",
       "      <td>0.751724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1309.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-860.0</td>\n",
       "      <td>10147.230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>435228.300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1454.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>525.0</td>\n",
       "      <td>15425.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>12150000.0</td>\n",
       "      <td>358386.75</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13594.50</td>\n",
       "      <td>0.050005</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.415385</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-1154.061538</td>\n",
       "      <td>1472756.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>405000.0</td>\n",
       "      <td>0.019689</td>\n",
       "      <td>-14469</td>\n",
       "      <td>-14437.0</td>\n",
       "      <td>-3992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Electricity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746644</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1673.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-665.000000</td>\n",
       "      <td>501661.710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  \\\n",
       "0      100002       1             0          202500.0    406597.5   \n",
       "1      100003       0             0          270000.0   1293502.5   \n",
       "2      100004       0             0           67500.0    135000.0   \n",
       "3      100006       0             0          135000.0    312682.5   \n",
       "4      100007       0             0          121500.0    513000.0   \n",
       "5      100008       0             0           99000.0    490495.5   \n",
       "6      100009       0             1          171000.0   1560726.0   \n",
       "7      100010       0             0          360000.0   1530000.0   \n",
       "8      100011       0             0          112500.0   1019610.0   \n",
       "9      100012       0             0          135000.0    405000.0   \n",
       "\n",
       "   REGION_POPULATION_RELATIVE  DAYS_BIRTH  DAYS_REGISTRATION  DAYS_ID_PUBLISH  \\\n",
       "0                    0.018801       -9461            -3648.0            -2120   \n",
       "1                    0.003541      -16765            -1186.0             -291   \n",
       "2                    0.010032      -19046            -4260.0            -2531   \n",
       "3                    0.008019      -19005            -9833.0            -2437   \n",
       "4                    0.028663      -19932            -4311.0            -3458   \n",
       "5                    0.035792      -16941            -4970.0             -477   \n",
       "6                    0.035792      -13778            -1213.0             -619   \n",
       "7                    0.003122      -18850            -4597.0            -2379   \n",
       "8                    0.018634      -20099            -7427.0            -3514   \n",
       "9                    0.019689      -14469           -14437.0            -3992   \n",
       "\n",
       "   OWN_CAR_AGE  FLAG_MOBIL  FLAG_EMP_PHONE  FLAG_WORK_PHONE  FLAG_CONT_MOBILE  \\\n",
       "0          NaN           1               1                0                 1   \n",
       "1          NaN           1               1                0                 1   \n",
       "2         26.0           1               1                1                 1   \n",
       "3          NaN           1               1                0                 1   \n",
       "4          NaN           1               1                0                 1   \n",
       "5          NaN           1               1                1                 1   \n",
       "6         17.0           1               1                0                 1   \n",
       "7          8.0           1               1                1                 1   \n",
       "8          NaN           1               0                0                 1   \n",
       "9          NaN           1               1                0                 1   \n",
       "\n",
       "   FLAG_PHONE  FLAG_EMAIL  REGION_RATING_CLIENT_W_CITY  \\\n",
       "0           1           0                            2   \n",
       "1           1           0                            1   \n",
       "2           1           0                            2   \n",
       "3           0           0                            2   \n",
       "4           0           0                            2   \n",
       "5           1           0                            2   \n",
       "6           1           0                            2   \n",
       "7           0           0                            3   \n",
       "8           0           0                            2   \n",
       "9           0           0                            2   \n",
       "\n",
       "   HOUR_APPR_PROCESS_START  REG_REGION_NOT_LIVE_REGION  \\\n",
       "0                       10                           0   \n",
       "1                       11                           0   \n",
       "2                        9                           0   \n",
       "3                       17                           0   \n",
       "4                       11                           0   \n",
       "5                       16                           0   \n",
       "6                       16                           0   \n",
       "7                       16                           0   \n",
       "8                       14                           0   \n",
       "9                        8                           0   \n",
       "\n",
       "   LIVE_REGION_NOT_WORK_REGION  REG_CITY_NOT_LIVE_CITY  \\\n",
       "0                            0                       0   \n",
       "1                            0                       0   \n",
       "2                            0                       0   \n",
       "3                            0                       0   \n",
       "4                            0                       0   \n",
       "5                            0                       0   \n",
       "6                            0                       0   \n",
       "7                            0                       0   \n",
       "8                            0                       0   \n",
       "9                            0                       0   \n",
       "\n",
       "   LIVE_CITY_NOT_WORK_CITY       ORGANIZATION_TYPE  EXT_SOURCE_1  \\\n",
       "0                        0  Business Entity Type 3      0.083037   \n",
       "1                        0                  School      0.311267   \n",
       "2                        0              Government           NaN   \n",
       "3                        0  Business Entity Type 3           NaN   \n",
       "4                        1                Religion           NaN   \n",
       "5                        0                   Other           NaN   \n",
       "6                        0  Business Entity Type 3      0.774761   \n",
       "7                        1                   Other           NaN   \n",
       "8                        0                     XNA      0.587334   \n",
       "9                        0             Electricity           NaN   \n",
       "\n",
       "   EXT_SOURCE_2  EXT_SOURCE_3  BASEMENTAREA_MEDI  \\\n",
       "0      0.262949      0.139376             0.0369   \n",
       "1      0.622246           NaN             0.0529   \n",
       "2      0.555912      0.729567                NaN   \n",
       "3      0.650442           NaN                NaN   \n",
       "4      0.322738           NaN                NaN   \n",
       "5      0.354225      0.621226                NaN   \n",
       "6      0.724000      0.492060                NaN   \n",
       "7      0.714279      0.540654                NaN   \n",
       "8      0.205747      0.751724                NaN   \n",
       "9      0.746644           NaN                NaN   \n",
       "\n",
       "   YEARS_BEGINEXPLUATATION_MEDI  YEARS_BUILD_MEDI  COMMONAREA_MEDI  \\\n",
       "0                        0.9722            0.6243           0.0144   \n",
       "1                        0.9851            0.7987           0.0608   \n",
       "2                           NaN               NaN              NaN   \n",
       "3                           NaN               NaN              NaN   \n",
       "4                           NaN               NaN              NaN   \n",
       "5                           NaN               NaN              NaN   \n",
       "6                           NaN               NaN              NaN   \n",
       "7                           NaN               NaN              NaN   \n",
       "8                           NaN               NaN              NaN   \n",
       "9                           NaN               NaN              NaN   \n",
       "\n",
       "   ENTRANCES_MEDI  FLOORSMAX_MEDI  LANDAREA_MEDI  NONLIVINGAPARTMENTS_MEDI  \\\n",
       "0          0.0690          0.0833         0.0375                    0.0000   \n",
       "1          0.0345          0.2917         0.0132                    0.0039   \n",
       "2             NaN             NaN            NaN                       NaN   \n",
       "3             NaN             NaN            NaN                       NaN   \n",
       "4             NaN             NaN            NaN                       NaN   \n",
       "5             NaN             NaN            NaN                       NaN   \n",
       "6             NaN             NaN            NaN                       NaN   \n",
       "7             NaN             NaN            NaN                       NaN   \n",
       "8             NaN             NaN            NaN                       NaN   \n",
       "9             NaN             NaN            NaN                       NaN   \n",
       "\n",
       "   NONLIVINGAREA_MEDI  TOTALAREA_MODE  OBS_60_CNT_SOCIAL_CIRCLE  \\\n",
       "0                0.00          0.0149                       2.0   \n",
       "1                0.01          0.0714                       1.0   \n",
       "2                 NaN             NaN                       0.0   \n",
       "3                 NaN             NaN                       2.0   \n",
       "4                 NaN             NaN                       0.0   \n",
       "5                 NaN             NaN                       0.0   \n",
       "6                 NaN             NaN                       1.0   \n",
       "7                 NaN             NaN                       2.0   \n",
       "8                 NaN             NaN                       1.0   \n",
       "9                 NaN             NaN                       2.0   \n",
       "\n",
       "   DEF_60_CNT_SOCIAL_CIRCLE  DAYS_LAST_PHONE_CHANGE  FLAG_DOCUMENT_2  \\\n",
       "0                       2.0                 -1134.0                0   \n",
       "1                       0.0                  -828.0                0   \n",
       "2                       0.0                  -815.0                0   \n",
       "3                       0.0                  -617.0                0   \n",
       "4                       0.0                 -1106.0                0   \n",
       "5                       0.0                 -2536.0                0   \n",
       "6                       0.0                 -1562.0                0   \n",
       "7                       0.0                 -1070.0                0   \n",
       "8                       0.0                     0.0                0   \n",
       "9                       0.0                 -1673.0                0   \n",
       "\n",
       "   FLAG_DOCUMENT_3  FLAG_DOCUMENT_4  FLAG_DOCUMENT_5  FLAG_DOCUMENT_6  \\\n",
       "0                1                0                0                0   \n",
       "1                1                0                0                0   \n",
       "2                0                0                0                0   \n",
       "3                1                0                0                0   \n",
       "4                0                0                0                0   \n",
       "5                1                0                0                0   \n",
       "6                0                0                0                0   \n",
       "7                1                0                0                0   \n",
       "8                1                0                0                0   \n",
       "9                0                0                0                0   \n",
       "\n",
       "   FLAG_DOCUMENT_7  FLAG_DOCUMENT_8  FLAG_DOCUMENT_9  FLAG_DOCUMENT_10  \\\n",
       "0                0                0                0                 0   \n",
       "1                0                0                0                 0   \n",
       "2                0                0                0                 0   \n",
       "3                0                0                0                 0   \n",
       "4                0                1                0                 0   \n",
       "5                0                0                0                 0   \n",
       "6                0                1                0                 0   \n",
       "7                0                0                0                 0   \n",
       "8                0                0                0                 0   \n",
       "9                0                0                0                 0   \n",
       "\n",
       "   FLAG_DOCUMENT_11  FLAG_DOCUMENT_12  FLAG_DOCUMENT_13  FLAG_DOCUMENT_14  \\\n",
       "0                 0                 0                 0                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "5                 0                 0                 0                 0   \n",
       "6                 0                 0                 0                 1   \n",
       "7                 0                 0                 0                 0   \n",
       "8                 0                 0                 0                 0   \n",
       "9                 0                 0                 0                 0   \n",
       "\n",
       "   FLAG_DOCUMENT_15  FLAG_DOCUMENT_16  FLAG_DOCUMENT_17  FLAG_DOCUMENT_18  \\\n",
       "0                 0                 0                 0                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "5                 0                 0                 0                 0   \n",
       "6                 0                 0                 0                 0   \n",
       "7                 0                 0                 0                 0   \n",
       "8                 0                 0                 0                 0   \n",
       "9                 0                 0                 0                 0   \n",
       "\n",
       "   FLAG_DOCUMENT_19  FLAG_DOCUMENT_20  FLAG_DOCUMENT_21  \\\n",
       "0                 0                 0                 0   \n",
       "1                 0                 0                 0   \n",
       "2                 0                 0                 0   \n",
       "3                 0                 0                 0   \n",
       "4                 0                 0                 0   \n",
       "5                 0                 0                 0   \n",
       "6                 0                 0                 0   \n",
       "7                 0                 0                 0   \n",
       "8                 0                 0                 0   \n",
       "9                 0                 0                 0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_HOUR  AMT_REQ_CREDIT_BUREAU_DAY  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         NaN                        NaN   \n",
       "4                         0.0                        0.0   \n",
       "5                         0.0                        0.0   \n",
       "6                         0.0                        0.0   \n",
       "7                         0.0                        0.0   \n",
       "8                         0.0                        0.0   \n",
       "9                         NaN                        NaN   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         NaN                        NaN   \n",
       "4                         0.0                        0.0   \n",
       "5                         0.0                        0.0   \n",
       "6                         0.0                        1.0   \n",
       "7                         0.0                        0.0   \n",
       "8                         0.0                        0.0   \n",
       "9                         NaN                        NaN   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  \\\n",
       "0                        0.0                         1.0   \n",
       "1                        0.0                         0.0   \n",
       "2                        0.0                         0.0   \n",
       "3                        NaN                         NaN   \n",
       "4                        0.0                         0.0   \n",
       "5                        1.0                         1.0   \n",
       "6                        1.0                         2.0   \n",
       "7                        0.0                         0.0   \n",
       "8                        0.0                         1.0   \n",
       "9                        NaN                         NaN   \n",
       "\n",
       "   BUREAU_DAYS_CREDIT_max  BUREAU_CREDIT_DAY_OVERDUE_sum  \\\n",
       "0                  -103.0                            0.0   \n",
       "1                  -606.0                            0.0   \n",
       "2                  -408.0                            0.0   \n",
       "3                     NaN                            NaN   \n",
       "4                 -1149.0                            0.0   \n",
       "5                   -78.0                            0.0   \n",
       "6                  -239.0                            0.0   \n",
       "7                 -1138.0                            0.0   \n",
       "8                 -1309.0                            0.0   \n",
       "9                     NaN                            NaN   \n",
       "\n",
       "   BUREAU_DAYS_CREDIT_ENDDATE_max  BUREAU_AMT_CREDIT_MAX_OVERDUE_max  \\\n",
       "0                           780.0                           5043.645   \n",
       "1                          1216.0                              0.000   \n",
       "2                          -382.0                              0.000   \n",
       "3                             NaN                                NaN   \n",
       "4                          -783.0                              0.000   \n",
       "5                           471.0                              0.000   \n",
       "6                          1402.0                              0.000   \n",
       "7                           689.0                                NaN   \n",
       "8                          -860.0                          10147.230   \n",
       "9                             NaN                                NaN   \n",
       "\n",
       "   BUREAU_CNT_CREDIT_PROLONG_sum  BUREAU_AMT_CREDIT_SUM_sum  \\\n",
       "0                            0.0                 865055.565   \n",
       "1                            0.0                1017400.500   \n",
       "2                            0.0                 189037.800   \n",
       "3                            NaN                        NaN   \n",
       "4                            0.0                 146250.000   \n",
       "5                            0.0                 468445.500   \n",
       "6                            0.0                4800811.500   \n",
       "7                            0.0                 990000.000   \n",
       "8                            0.0                 435228.300   \n",
       "9                            NaN                        NaN   \n",
       "\n",
       "   BUREAU_AMT_CREDIT_SUM_DEBT_sum  BUREAU_AMT_CREDIT_SUM_OVERDUE_mean  \\\n",
       "0                        245781.0                                 0.0   \n",
       "1                             0.0                                 0.0   \n",
       "2                             0.0                                 0.0   \n",
       "3                             NaN                                 NaN   \n",
       "4                             0.0                                 0.0   \n",
       "5                        240057.0                                 0.0   \n",
       "6                       1077349.5                                 0.0   \n",
       "7                        348007.5                                 0.0   \n",
       "8                             0.0                                 0.0   \n",
       "9                             NaN                                 NaN   \n",
       "\n",
       "   BUREAU_DAYS_CREDIT_UPDATE_mean  BUREAU_AMT_ANNUITY_sum  ca_Active  \\\n",
       "0                     -499.875000                     0.0        2.0   \n",
       "1                     -816.000000                     0.0        1.0   \n",
       "2                     -532.000000                     0.0        0.0   \n",
       "3                             NaN                     NaN        NaN   \n",
       "4                     -783.000000                     0.0        0.0   \n",
       "5                     -611.000000                     0.0        1.0   \n",
       "6                     -851.611111                     0.0        4.0   \n",
       "7                     -578.000000                     0.0        1.0   \n",
       "8                    -1454.750000                     0.0        0.0   \n",
       "9                             NaN                     NaN        NaN   \n",
       "\n",
       "   ca_Bad debt  ca_Sold  ct_Another type of loan  ct_Car loan  \\\n",
       "0          0.0      0.0                      0.0          0.0   \n",
       "1          0.0      0.0                      0.0          0.0   \n",
       "2          0.0      0.0                      0.0          0.0   \n",
       "3          NaN      NaN                      NaN          NaN   \n",
       "4          0.0      0.0                      0.0          0.0   \n",
       "5          0.0      0.0                      0.0          0.0   \n",
       "6          0.0      0.0                      0.0          0.0   \n",
       "7          0.0      0.0                      0.0          0.0   \n",
       "8          0.0      0.0                      0.0          0.0   \n",
       "9          NaN      NaN                      NaN          NaN   \n",
       "\n",
       "   ct_Cash loan (non-earmarked)  ct_Consumer credit  ct_Credit card  \\\n",
       "0                           0.0                 4.0             4.0   \n",
       "1                           0.0                 2.0             2.0   \n",
       "2                           0.0                 2.0             0.0   \n",
       "3                           NaN                 NaN             NaN   \n",
       "4                           0.0                 1.0             0.0   \n",
       "5                           0.0                 3.0             0.0   \n",
       "6                           0.0                16.0             2.0   \n",
       "7                           0.0                 1.0             0.0   \n",
       "8                           0.0                 3.0             1.0   \n",
       "9                           NaN                 NaN             NaN   \n",
       "\n",
       "   ct_Interbank credit  ct_Loan for business development  \\\n",
       "0                  0.0                               0.0   \n",
       "1                  0.0                               0.0   \n",
       "2                  0.0                               0.0   \n",
       "3                  NaN                               NaN   \n",
       "4                  0.0                               0.0   \n",
       "5                  0.0                               0.0   \n",
       "6                  0.0                               0.0   \n",
       "7                  0.0                               1.0   \n",
       "8                  0.0                               0.0   \n",
       "9                  NaN                               NaN   \n",
       "\n",
       "   ct_Loan for purchase of shares (margin lending)  \\\n",
       "0                                              0.0   \n",
       "1                                              0.0   \n",
       "2                                              0.0   \n",
       "3                                              NaN   \n",
       "4                                              0.0   \n",
       "5                                              0.0   \n",
       "6                                              0.0   \n",
       "7                                              0.0   \n",
       "8                                              0.0   \n",
       "9                                              NaN   \n",
       "\n",
       "   ct_Loan for the purchase of equipment  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    NaN   \n",
       "4                                    0.0   \n",
       "5                                    0.0   \n",
       "6                                    0.0   \n",
       "7                                    0.0   \n",
       "8                                    0.0   \n",
       "9                                    NaN   \n",
       "\n",
       "   ct_Loan for working capital replenishment  ct_Microloan  \\\n",
       "0                                        0.0           0.0   \n",
       "1                                        0.0           0.0   \n",
       "2                                        0.0           0.0   \n",
       "3                                        NaN           NaN   \n",
       "4                                        0.0           0.0   \n",
       "5                                        0.0           0.0   \n",
       "6                                        0.0           0.0   \n",
       "7                                        0.0           0.0   \n",
       "8                                        0.0           0.0   \n",
       "9                                        NaN           NaN   \n",
       "\n",
       "   ct_Mobile operator loan  ct_Mortgage  ct_Real estate loan  \\\n",
       "0                      0.0          0.0                  0.0   \n",
       "1                      0.0          0.0                  0.0   \n",
       "2                      0.0          0.0                  0.0   \n",
       "3                      NaN          NaN                  NaN   \n",
       "4                      0.0          0.0                  0.0   \n",
       "5                      0.0          0.0                  0.0   \n",
       "6                      0.0          0.0                  0.0   \n",
       "7                      0.0          0.0                  0.0   \n",
       "8                      0.0          0.0                  0.0   \n",
       "9                      NaN          NaN                  NaN   \n",
       "\n",
       "   ct_Unknown type of loan  MONTHS_BALANCE_max  CNT_INSTALMENT_FUTURE_sum  \\\n",
       "0                      0.0                -1.0                      285.0   \n",
       "1                      0.0               -18.0                      162.0   \n",
       "2                      0.0               -24.0                        9.0   \n",
       "3                      NaN                -1.0                      173.0   \n",
       "4                      0.0                -1.0                      592.0   \n",
       "5                      0.0                -2.0                      341.0   \n",
       "6                      0.0                -1.0                      242.0   \n",
       "7                      0.0               -25.0                       55.0   \n",
       "8                      0.0               -10.0                      525.0   \n",
       "9                      NaN                -5.0                      470.0   \n",
       "\n",
       "   SK_DPD_sum_x  SK_DPD_DEF_sum_x  AMT_CREDIT_LIMIT_ACTUAL_sum  \\\n",
       "0           0.0               0.0                          NaN   \n",
       "1           0.0               0.0                          NaN   \n",
       "2           0.0               0.0                          NaN   \n",
       "3           0.0               0.0                    1620000.0   \n",
       "4           0.0               0.0                          NaN   \n",
       "5       28142.0               0.0                          NaN   \n",
       "6           0.0               0.0                          NaN   \n",
       "7           0.0               0.0                          NaN   \n",
       "8       15425.0              26.0                   12150000.0   \n",
       "9           0.0               0.0                          NaN   \n",
       "\n",
       "   AMT_PAYMENT_CURRENT_sum  CNT_DRAWINGS_CURRENT_sum  SK_DPD_sum_y  \\\n",
       "0                      NaN                       NaN           NaN   \n",
       "1                      NaN                       NaN           NaN   \n",
       "2                      NaN                       NaN           NaN   \n",
       "3                     0.00                       0.0           0.0   \n",
       "4                      NaN                       NaN           NaN   \n",
       "5                      NaN                       NaN           NaN   \n",
       "6                      NaN                       NaN           NaN   \n",
       "7                      NaN                       NaN           NaN   \n",
       "8                358386.75                       4.0           0.0   \n",
       "9                      NaN                       NaN           NaN   \n",
       "\n",
       "   SK_DPD_DEF_sum_y  AMT_DOWN_PAYMENT_sum  RATE_DOWN_PAYMENT_mean  \\\n",
       "0               NaN                  0.00                0.000000   \n",
       "1               NaN               6885.00                0.050030   \n",
       "2               NaN               4860.00                0.212008   \n",
       "3               0.0              69680.34                0.163412   \n",
       "4               NaN               6781.50                0.159516   \n",
       "5               NaN              16645.50                0.073051   \n",
       "6               NaN              64422.00                0.126602   \n",
       "7               NaN                  0.00                0.000000   \n",
       "8               0.0              13594.50                0.050005   \n",
       "9               NaN                  0.00                0.000000   \n",
       "\n",
       "   CNT_PAYMENT_sum  INST_NUM_INSTALMENT_VERSION_mean  \\\n",
       "0             24.0                          1.052632   \n",
       "1             30.0                          1.040000   \n",
       "2              4.0                          1.333333   \n",
       "3            138.0                          1.125000   \n",
       "4            124.0                          1.166667   \n",
       "5             56.0                          1.028571   \n",
       "6             56.0                          1.000000   \n",
       "7             10.0                          1.000000   \n",
       "8             42.0                          0.415385   \n",
       "9             54.0                          2.000000   \n",
       "\n",
       "   INST_NUM_INSTALMENT_NUMBER_max  INST_DAYS_INSTALMENT_mean  \\\n",
       "0                            19.0                -295.000000   \n",
       "1                            12.0               -1378.160000   \n",
       "2                             3.0                -754.000000   \n",
       "3                            10.0                -252.250000   \n",
       "4                            17.0               -1028.606061   \n",
       "5                            10.0               -1263.914286   \n",
       "6                            12.0                -855.823529   \n",
       "7                            10.0                -904.000000   \n",
       "8                            76.0               -1154.061538   \n",
       "9                            23.0                -665.000000   \n",
       "\n",
       "   INST_AMT_PAYMENT_sum  \n",
       "0            219625.695  \n",
       "1           1618864.650  \n",
       "2             21288.465  \n",
       "3           1007153.415  \n",
       "4            806127.975  \n",
       "5            957617.595  \n",
       "6            487995.120  \n",
       "7            274492.080  \n",
       "8           1472756.175  \n",
       "9            501661.710  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_optimized_num_cat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1481b0-758e-4b68-bad1-14481f32892d",
   "metadata": {},
   "source": [
    "## faire suivre l'optimisation sur test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "875a9c3d-f901-40db-966a-3777b1ae901e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48744, 108)\n"
     ]
    }
   ],
   "source": [
    "# Étape 1 : Identifier les variables à conserver\n",
    "columns_to_keep = train_optimized_num_cat.columns.tolist()\n",
    "if 'TARGET' in columns_to_keep:\n",
    "    columns_to_keep.remove('TARGET')\n",
    "\n",
    "# Étape 2 : Appliquer la sélection au DataFrame test\n",
    "test_optimized_num_cat = test[columns_to_keep]\n",
    "\n",
    "# Étape 3 : Vérification\n",
    "print(test_optimized_num_cat.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80542942-f03f-46b9-8ef1-1046a115e5c6",
   "metadata": {},
   "source": [
    "presence_rate(test_optimized_num_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc90da23-994f-4487-bbf4-3be25a9dea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install dtale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f378b-c14e-4d1e-97cd-b80746db85cc",
   "metadata": {},
   "source": [
    "import dtale\n",
    "dtale.show(train_optimized_num_cat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4299a56-4ee1-4efe-8f26-b92535cf9af5",
   "metadata": {},
   "source": [
    "# Convertir les variables days en years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d1a8c9a-da6a-4b1c-b0bb-5ab0b3e3072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_days_to_years_and_rename(df, columns):\n",
    "    \"\"\"\n",
    "    Convertit les valeurs en jours en années pour les colonnes spécifiées dans un DataFrame et renomme ces colonnes.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame contenant les données.\n",
    "    columns (list): Liste des noms de colonnes à convertir.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame avec les colonnes converties et renommées.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        new_col_name = col.replace('DAYS_', 'YEARS_')\n",
    "        # Convertir les jours en années, en évitant les valeurs aberrantes pour DAYS_EMPLOYED\n",
    "        if col == 'DAYS_EMPLOYED':\n",
    "            df[new_col_name] = df[col].apply(lambda x: x / -365 if x < 0 else 0)\n",
    "        else:\n",
    "            df[new_col_name] = df[col].apply(lambda x: x / -365)\n",
    "        df.drop(columns=[col], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bde4465e-cf89-42bd-9bd5-312d2dcfd277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables représentant des nombres de jours: ['DAYS_BIRTH', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'DAYS_LAST_PHONE_CHANGE', 'BUREAU_DAYS_CREDIT_max', 'BUREAU_DAYS_CREDIT_ENDDATE_max', 'BUREAU_DAYS_CREDIT_UPDATE_mean', 'INST_DAYS_INSTALMENT_mean']\n"
     ]
    }
   ],
   "source": [
    "day_variables = [col for col in train_optimized_num_cat.columns if 'DAYS' in col or 'DATE' in col]\n",
    "print(\"Variables représentant des nombres de jours:\", day_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04d791a4-4575-4a33-935f-2d841a369c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "columns_to_convert = day_variables\n",
    "train = convert_days_to_years_and_rename(train, columns_to_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11910298-0152-408c-b0de-cf8f4498e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "columns_to_convert = day_variables\n",
    "test = convert_days_to_years_and_rename(test, columns_to_convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ada95f-4d00-4906-a9bb-04efcd792ad4",
   "metadata": {},
   "source": [
    "# Traiter les variables à nombre de valeurs fini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "075e8172-3f33-492b-ab84-541b923298c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_unique_values_variables(df):\n",
    "\n",
    "    # Créer un dictionnaire pour stocker le nombre de valeurs uniques pour chaque variable\n",
    "    unique_counts = {col: df[col].nunique() for col in df.columns}\n",
    "    \n",
    "    # Filtrer pour obtenir les variables avec un nombre fini de valeurs uniques\n",
    "    # Vous pouvez définir un seuil spécifique si nécessaire, par exemple, moins de 20 valeurs uniques\n",
    "    finite_vars = {col: count for col, count in unique_counts.items() if count < 20}  # Exemple de seuil: 20\n",
    "    \n",
    "    # Afficher la liste des variables à nombre fini de valeurs uniques\n",
    "    print(finite_vars)\n",
    "\n",
    "    return finite_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "973526d8-4bb2-4eae-9762-05b2ed485314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TARGET': 2, 'NAME_CONTRACT_TYPE': 2, 'CODE_GENDER': 3, 'FLAG_OWN_CAR': 2, 'FLAG_OWN_REALTY': 2, 'CNT_CHILDREN': 15, 'NAME_TYPE_SUITE': 7, 'NAME_INCOME_TYPE': 8, 'NAME_EDUCATION_TYPE': 5, 'NAME_FAMILY_STATUS': 6, 'NAME_HOUSING_TYPE': 6, 'FLAG_MOBIL': 2, 'FLAG_EMP_PHONE': 2, 'FLAG_WORK_PHONE': 2, 'FLAG_CONT_MOBILE': 2, 'FLAG_PHONE': 2, 'FLAG_EMAIL': 2, 'OCCUPATION_TYPE': 18, 'CNT_FAM_MEMBERS': 17, 'REGION_RATING_CLIENT': 3, 'REGION_RATING_CLIENT_W_CITY': 3, 'WEEKDAY_APPR_PROCESS_START': 7, 'REG_REGION_NOT_LIVE_REGION': 2, 'REG_REGION_NOT_WORK_REGION': 2, 'LIVE_REGION_NOT_WORK_REGION': 2, 'REG_CITY_NOT_LIVE_CITY': 2, 'REG_CITY_NOT_WORK_CITY': 2, 'LIVE_CITY_NOT_WORK_CITY': 2, 'FONDKAPREMONT_MODE': 4, 'HOUSETYPE_MODE': 3, 'WALLSMATERIAL_MODE': 7, 'EMERGENCYSTATE_MODE': 2, 'DEF_30_CNT_SOCIAL_CIRCLE': 10, 'DEF_60_CNT_SOCIAL_CIRCLE': 9, 'FLAG_DOCUMENT_2': 2, 'FLAG_DOCUMENT_3': 2, 'FLAG_DOCUMENT_4': 2, 'FLAG_DOCUMENT_5': 2, 'FLAG_DOCUMENT_6': 2, 'FLAG_DOCUMENT_7': 2, 'FLAG_DOCUMENT_8': 2, 'FLAG_DOCUMENT_9': 2, 'FLAG_DOCUMENT_10': 2, 'FLAG_DOCUMENT_11': 2, 'FLAG_DOCUMENT_12': 2, 'FLAG_DOCUMENT_13': 2, 'FLAG_DOCUMENT_14': 2, 'FLAG_DOCUMENT_15': 2, 'FLAG_DOCUMENT_16': 2, 'FLAG_DOCUMENT_17': 2, 'FLAG_DOCUMENT_18': 2, 'FLAG_DOCUMENT_19': 2, 'FLAG_DOCUMENT_20': 2, 'FLAG_DOCUMENT_21': 2, 'AMT_REQ_CREDIT_BUREAU_HOUR': 5, 'AMT_REQ_CREDIT_BUREAU_DAY': 9, 'AMT_REQ_CREDIT_BUREAU_WEEK': 9, 'AMT_REQ_CREDIT_BUREAU_QRT': 11, 'BUREAU_CNT_CREDIT_PROLONG_sum': 10, 'ca_Bad debt': 2, 'ca_Sold': 8, 'ct_Another type of loan': 5, 'ct_Car loan': 10, 'ct_Cash loan (non-earmarked)': 3, 'ct_Interbank credit': 2, 'ct_Loan for business development': 8, 'ct_Loan for purchase of shares (margin lending)': 2, 'ct_Loan for the purchase of equipment': 3, 'ct_Loan for working capital replenishment': 5, 'ct_Mobile operator loan': 2, 'ct_Mortgage': 8, 'ct_Real estate loan': 2, 'ct_Unknown type of loan': 5, 'MOST_FREQ_CURRENCY': 3}\n"
     ]
    }
   ],
   "source": [
    "finite_vars=finite_unique_values_variables(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88fa7c4a-98fc-4f40-b0b2-537e9b4c0eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finite_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "242d1a03-d623-4a93-89fd-cf5be4be6722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_single_value_columns(df):\n",
    "    \"\"\"\n",
    "    Supprime les colonnes du DataFrame qui contiennent une seule valeur unique.\n",
    "\n",
    "    :param df: DataFrame à nettoyer.\n",
    "    :return: DataFrame sans les colonnes à valeur unique.\n",
    "    \"\"\"\n",
    "    # Identifier les colonnes qui ont une seule valeur unique\n",
    "    single_value_columns = [col for col in df.columns if df[col].nunique() == 1]\n",
    "    \n",
    "    # Supprimer ces colonnes\n",
    "    df = df.drop(columns=single_value_columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Pour l'utiliser, appelez la fonction avec votre DataFrame :\n",
    "# train = remove_single_value_columns(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a87e0be-32cc-4311-b568-54e5011e9a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finite_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3273850c-4455-434c-9f7e-33c7750bf75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 192)\n"
     ]
    }
   ],
   "source": [
    "# Appliquer la fonction à votre DataFrame\n",
    "train = remove_single_value_columns(train)\n",
    "\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945cabc-7852-4dfa-bb62-a1d496d1a8a2",
   "metadata": {},
   "source": [
    "# Impacter sur test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06e252ec-4403-4a81-a0fb-f3af4d225e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48744, 191)\n"
     ]
    }
   ],
   "source": [
    "# Étape 1 : Identifier les variables à conserver\n",
    "columns_to_keep = train.columns.tolist()\n",
    "if 'TARGET' in columns_to_keep:\n",
    "    columns_to_keep.remove('TARGET')\n",
    "\n",
    "# Étape 2 : Appliquer la sélection au DataFrame test\n",
    "test = test[columns_to_keep]\n",
    "\n",
    "# Étape 3 : Vérification\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c12e393-f7ca-4015-a6ad-23a163bf3b42",
   "metadata": {},
   "source": [
    "# Reduire le nombre d'enfants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dca1236-7a85-405a-888b-4d4aca3ce0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306956, 192)\n"
     ]
    }
   ],
   "source": [
    "# Supprimer les entrées avec plus de 3 enfants\n",
    "train = train[train['CNT_CHILDREN'] <= 3]\n",
    "print(train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3420bb95-aa55-4e93-b410-f6bff4e3695e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48673, 191)\n"
     ]
    }
   ],
   "source": [
    "# Supprimer les entrées avec plus de 3 enfants\n",
    "test = test[test['CNT_CHILDREN'] <= 3]\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f81ccc2-cd68-4458-aa95-ee43d085affa",
   "metadata": {},
   "source": [
    "# Transformation des variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2121f20b-2d46-4a05-834c-cc4a9cec51e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetrical_log_transform(df, monetary_vars, small_value=1e-6):\n",
    "    for var in monetary_vars:\n",
    "        if var in df.columns:\n",
    "            # Appliquer une transformation symétrique du logarithme\n",
    "            df[var + '_log'] = np.sign(df[var]) * np.log(np.abs(df[var]) + small_value)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d6be5ad-37fb-440b-8605-3817bcbe5e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, cat_vars):\n",
    "    return pd.get_dummies(df, columns=cat_vars, dummy_na=True, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "663d33d7-ddfa-43b7-9ac2-6f63002eafe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des variables catégorielles\n",
    "cat_vars = train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Appliquer l'encodage One-Hot\n",
    "train = one_hot_encode(train, cat_vars)\n",
    "test = one_hot_encode(test, cat_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1f4731d-9c04-4b38-a219-8936039b1bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306956, 318)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee636dab-5d9c-4c3b-ace3-f95ae1f42c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables représentant des montants d'argent: ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR', 'BUREAU_DAYS_CREDIT_max', 'BUREAU_CREDIT_DAY_OVERDUE_sum', 'BUREAU_DAYS_CREDIT_ENDDATE_max', 'BUREAU_AMT_CREDIT_MAX_OVERDUE_max', 'BUREAU_CNT_CREDIT_PROLONG_sum', 'BUREAU_AMT_CREDIT_SUM_sum', 'BUREAU_AMT_CREDIT_SUM_DEBT_sum', 'BUREAU_AMT_CREDIT_SUM_OVERDUE_mean', 'BUREAU_DAYS_CREDIT_UPDATE_mean', 'BUREAU_AMT_ANNUITY_sum', 'AMT_CREDIT_LIMIT_ACTUAL_sum', 'AMT_PAYMENT_CURRENT_sum', 'AMT_DOWN_PAYMENT_sum', 'INST_AMT_PAYMENT_sum']\n"
     ]
    }
   ],
   "source": [
    "money_variables = [col for col in train_optimized_num_cat.columns if 'AMT' in col or 'INCOME' in col or 'CREDIT' in col or 'ANNUITY' in col]\n",
    "print(\"Variables représentant des montants d'argent:\", money_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc808697-668a-492f-8148-482f349bae02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TARGET': 2, 'CNT_CHILDREN': 4, 'FLAG_MOBIL': 2, 'FLAG_EMP_PHONE': 2, 'FLAG_WORK_PHONE': 2, 'FLAG_CONT_MOBILE': 2, 'FLAG_PHONE': 2, 'FLAG_EMAIL': 2, 'CNT_FAM_MEMBERS': 5, 'REGION_RATING_CLIENT': 3, 'REGION_RATING_CLIENT_W_CITY': 3, 'REG_REGION_NOT_LIVE_REGION': 2, 'REG_REGION_NOT_WORK_REGION': 2, 'LIVE_REGION_NOT_WORK_REGION': 2, 'REG_CITY_NOT_LIVE_CITY': 2, 'REG_CITY_NOT_WORK_CITY': 2, 'LIVE_CITY_NOT_WORK_CITY': 2, 'DEF_30_CNT_SOCIAL_CIRCLE': 10, 'DEF_60_CNT_SOCIAL_CIRCLE': 9, 'FLAG_DOCUMENT_2': 2, 'FLAG_DOCUMENT_3': 2, 'FLAG_DOCUMENT_4': 2, 'FLAG_DOCUMENT_5': 2, 'FLAG_DOCUMENT_6': 2, 'FLAG_DOCUMENT_7': 2, 'FLAG_DOCUMENT_8': 2, 'FLAG_DOCUMENT_9': 2, 'FLAG_DOCUMENT_10': 2, 'FLAG_DOCUMENT_11': 2, 'FLAG_DOCUMENT_12': 2, 'FLAG_DOCUMENT_13': 2, 'FLAG_DOCUMENT_14': 2, 'FLAG_DOCUMENT_15': 2, 'FLAG_DOCUMENT_16': 2, 'FLAG_DOCUMENT_17': 2, 'FLAG_DOCUMENT_18': 2, 'FLAG_DOCUMENT_19': 2, 'FLAG_DOCUMENT_20': 2, 'FLAG_DOCUMENT_21': 2, 'AMT_REQ_CREDIT_BUREAU_HOUR': 5, 'AMT_REQ_CREDIT_BUREAU_DAY': 9, 'AMT_REQ_CREDIT_BUREAU_WEEK': 9, 'AMT_REQ_CREDIT_BUREAU_QRT': 11, 'BUREAU_CNT_CREDIT_PROLONG_sum': 10, 'ca_Bad debt': 2, 'ca_Sold': 8, 'ct_Another type of loan': 5, 'ct_Car loan': 10, 'ct_Cash loan (non-earmarked)': 3, 'ct_Interbank credit': 2, 'ct_Loan for business development': 8, 'ct_Loan for purchase of shares (margin lending)': 2, 'ct_Loan for the purchase of equipment': 3, 'ct_Loan for working capital replenishment': 5, 'ct_Mobile operator loan': 2, 'ct_Mortgage': 8, 'ct_Real estate loan': 2, 'ct_Unknown type of loan': 5, 'NAME_CONTRACT_TYPE_Revolving loans': 2, 'NAME_CONTRACT_TYPE_nan': 1, 'CODE_GENDER_M': 2, 'CODE_GENDER_XNA': 2, 'CODE_GENDER_nan': 1, 'FLAG_OWN_CAR_Y': 2, 'FLAG_OWN_CAR_nan': 1, 'FLAG_OWN_REALTY_Y': 2, 'FLAG_OWN_REALTY_nan': 1, 'NAME_TYPE_SUITE_Family': 2, 'NAME_TYPE_SUITE_Group of people': 2, 'NAME_TYPE_SUITE_Other_A': 2, 'NAME_TYPE_SUITE_Other_B': 2, 'NAME_TYPE_SUITE_Spouse, partner': 2, 'NAME_TYPE_SUITE_Unaccompanied': 2, 'NAME_TYPE_SUITE_nan': 2, 'NAME_INCOME_TYPE_Commercial associate': 2, 'NAME_INCOME_TYPE_Maternity leave': 2, 'NAME_INCOME_TYPE_Pensioner': 2, 'NAME_INCOME_TYPE_State servant': 2, 'NAME_INCOME_TYPE_Student': 2, 'NAME_INCOME_TYPE_Unemployed': 2, 'NAME_INCOME_TYPE_Working': 2, 'NAME_INCOME_TYPE_nan': 1, 'NAME_EDUCATION_TYPE_Higher education': 2, 'NAME_EDUCATION_TYPE_Incomplete higher': 2, 'NAME_EDUCATION_TYPE_Lower secondary': 2, 'NAME_EDUCATION_TYPE_Secondary / secondary special': 2, 'NAME_EDUCATION_TYPE_nan': 1, 'NAME_FAMILY_STATUS_Married': 2, 'NAME_FAMILY_STATUS_Separated': 2, 'NAME_FAMILY_STATUS_Single / not married': 2, 'NAME_FAMILY_STATUS_Unknown': 2, 'NAME_FAMILY_STATUS_Widow': 2, 'NAME_FAMILY_STATUS_nan': 1, 'NAME_HOUSING_TYPE_House / apartment': 2, 'NAME_HOUSING_TYPE_Municipal apartment': 2, 'NAME_HOUSING_TYPE_Office apartment': 2, 'NAME_HOUSING_TYPE_Rented apartment': 2, 'NAME_HOUSING_TYPE_With parents': 2, 'NAME_HOUSING_TYPE_nan': 1, 'OCCUPATION_TYPE_Cleaning staff': 2, 'OCCUPATION_TYPE_Cooking staff': 2, 'OCCUPATION_TYPE_Core staff': 2, 'OCCUPATION_TYPE_Drivers': 2, 'OCCUPATION_TYPE_HR staff': 2, 'OCCUPATION_TYPE_High skill tech staff': 2, 'OCCUPATION_TYPE_IT staff': 2, 'OCCUPATION_TYPE_Laborers': 2, 'OCCUPATION_TYPE_Low-skill Laborers': 2, 'OCCUPATION_TYPE_Managers': 2, 'OCCUPATION_TYPE_Medicine staff': 2, 'OCCUPATION_TYPE_Private service staff': 2, 'OCCUPATION_TYPE_Realty agents': 2, 'OCCUPATION_TYPE_Sales staff': 2, 'OCCUPATION_TYPE_Secretaries': 2, 'OCCUPATION_TYPE_Security staff': 2, 'OCCUPATION_TYPE_Waiters/barmen staff': 2, 'OCCUPATION_TYPE_nan': 2, 'WEEKDAY_APPR_PROCESS_START_MONDAY': 2, 'WEEKDAY_APPR_PROCESS_START_SATURDAY': 2, 'WEEKDAY_APPR_PROCESS_START_SUNDAY': 2, 'WEEKDAY_APPR_PROCESS_START_THURSDAY': 2, 'WEEKDAY_APPR_PROCESS_START_TUESDAY': 2, 'WEEKDAY_APPR_PROCESS_START_WEDNESDAY': 2, 'WEEKDAY_APPR_PROCESS_START_nan': 1, 'ORGANIZATION_TYPE_Agriculture': 2, 'ORGANIZATION_TYPE_Bank': 2, 'ORGANIZATION_TYPE_Business Entity Type 1': 2, 'ORGANIZATION_TYPE_Business Entity Type 2': 2, 'ORGANIZATION_TYPE_Business Entity Type 3': 2, 'ORGANIZATION_TYPE_Cleaning': 2, 'ORGANIZATION_TYPE_Construction': 2, 'ORGANIZATION_TYPE_Culture': 2, 'ORGANIZATION_TYPE_Electricity': 2, 'ORGANIZATION_TYPE_Emergency': 2, 'ORGANIZATION_TYPE_Government': 2, 'ORGANIZATION_TYPE_Hotel': 2, 'ORGANIZATION_TYPE_Housing': 2, 'ORGANIZATION_TYPE_Industry: type 1': 2, 'ORGANIZATION_TYPE_Industry: type 10': 2, 'ORGANIZATION_TYPE_Industry: type 11': 2, 'ORGANIZATION_TYPE_Industry: type 12': 2, 'ORGANIZATION_TYPE_Industry: type 13': 2, 'ORGANIZATION_TYPE_Industry: type 2': 2, 'ORGANIZATION_TYPE_Industry: type 3': 2, 'ORGANIZATION_TYPE_Industry: type 4': 2, 'ORGANIZATION_TYPE_Industry: type 5': 2, 'ORGANIZATION_TYPE_Industry: type 6': 2, 'ORGANIZATION_TYPE_Industry: type 7': 2, 'ORGANIZATION_TYPE_Industry: type 8': 2, 'ORGANIZATION_TYPE_Industry: type 9': 2, 'ORGANIZATION_TYPE_Insurance': 2, 'ORGANIZATION_TYPE_Kindergarten': 2, 'ORGANIZATION_TYPE_Legal Services': 2, 'ORGANIZATION_TYPE_Medicine': 2, 'ORGANIZATION_TYPE_Military': 2, 'ORGANIZATION_TYPE_Mobile': 2, 'ORGANIZATION_TYPE_Other': 2, 'ORGANIZATION_TYPE_Police': 2, 'ORGANIZATION_TYPE_Postal': 2, 'ORGANIZATION_TYPE_Realtor': 2, 'ORGANIZATION_TYPE_Religion': 2, 'ORGANIZATION_TYPE_Restaurant': 2, 'ORGANIZATION_TYPE_School': 2, 'ORGANIZATION_TYPE_Security': 2, 'ORGANIZATION_TYPE_Security Ministries': 2, 'ORGANIZATION_TYPE_Self-employed': 2, 'ORGANIZATION_TYPE_Services': 2, 'ORGANIZATION_TYPE_Telecom': 2, 'ORGANIZATION_TYPE_Trade: type 1': 2, 'ORGANIZATION_TYPE_Trade: type 2': 2, 'ORGANIZATION_TYPE_Trade: type 3': 2, 'ORGANIZATION_TYPE_Trade: type 4': 2, 'ORGANIZATION_TYPE_Trade: type 5': 2, 'ORGANIZATION_TYPE_Trade: type 6': 2, 'ORGANIZATION_TYPE_Trade: type 7': 2, 'ORGANIZATION_TYPE_Transport: type 1': 2, 'ORGANIZATION_TYPE_Transport: type 2': 2, 'ORGANIZATION_TYPE_Transport: type 3': 2, 'ORGANIZATION_TYPE_Transport: type 4': 2, 'ORGANIZATION_TYPE_University': 2, 'ORGANIZATION_TYPE_XNA': 2, 'ORGANIZATION_TYPE_nan': 1, 'FONDKAPREMONT_MODE_org spec account': 2, 'FONDKAPREMONT_MODE_reg oper account': 2, 'FONDKAPREMONT_MODE_reg oper spec account': 2, 'FONDKAPREMONT_MODE_nan': 2, 'HOUSETYPE_MODE_specific housing': 2, 'HOUSETYPE_MODE_terraced house': 2, 'HOUSETYPE_MODE_nan': 2, 'WALLSMATERIAL_MODE_Mixed': 2, 'WALLSMATERIAL_MODE_Monolithic': 2, 'WALLSMATERIAL_MODE_Others': 2, 'WALLSMATERIAL_MODE_Panel': 2, 'WALLSMATERIAL_MODE_Stone, brick': 2, 'WALLSMATERIAL_MODE_Wooden': 2, 'WALLSMATERIAL_MODE_nan': 2, 'EMERGENCYSTATE_MODE_Yes': 2, 'EMERGENCYSTATE_MODE_nan': 2, 'MOST_FREQ_CURRENCY_currency 2': 2, 'MOST_FREQ_CURRENCY_currency 3': 2, 'MOST_FREQ_CURRENCY_nan': 2}\n"
     ]
    }
   ],
   "source": [
    "finite_vars=finite_unique_values_variables(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fb1f87f-7288-4b84-97ae-e41a525d46ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables représentant des montants d'argent: ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR', 'BUREAU_DAYS_CREDIT_min', 'BUREAU_DAYS_CREDIT_mean', 'BUREAU_CREDIT_DAY_OVERDUE_max', 'BUREAU_CREDIT_DAY_OVERDUE_sum', 'BUREAU_AMT_CREDIT_MAX_OVERDUE_mean', 'BUREAU_AMT_CREDIT_MAX_OVERDUE_max', 'BUREAU_CNT_CREDIT_PROLONG_sum', 'BUREAU_AMT_CREDIT_SUM_sum', 'BUREAU_AMT_CREDIT_SUM_DEBT_sum', 'BUREAU_AMT_CREDIT_SUM_OVERDUE_mean', 'BUREAU_AMT_ANNUITY_mean', 'BUREAU_AMT_ANNUITY_sum', 'AMT_BALANCE_sum', 'AMT_CREDIT_LIMIT_ACTUAL_sum', 'AMT_DRAWINGS_ATM_CURRENT_sum', 'AMT_DRAWINGS_CURRENT_sum', 'AMT_PAYMENT_CURRENT_sum', 'AMT_ANNUITY_sum', 'AMT_APPLICATION_sum', 'AMT_CREDIT_sum', 'AMT_DOWN_PAYMENT_sum', 'AMT_GOODS_PRICE_sum', 'INST_AMT_INSTALMENT_sum', 'INST_AMT_PAYMENT_sum', 'BUREAU_YEARS_CREDIT_max', 'BUREAU_YEARS_CREDIT_ENDDATE_max', 'BUREAU_YEARS_CREDIT_UPDATE_mean', 'NAME_INCOME_TYPE_Commercial associate', 'NAME_INCOME_TYPE_Maternity leave', 'NAME_INCOME_TYPE_Pensioner', 'NAME_INCOME_TYPE_State servant', 'NAME_INCOME_TYPE_Student', 'NAME_INCOME_TYPE_Unemployed', 'NAME_INCOME_TYPE_Working', 'NAME_INCOME_TYPE_nan']\n"
     ]
    }
   ],
   "source": [
    "money_variables = [col for col in train.columns if 'AMT' in col or 'INCOME' in col or 'CREDIT' in col or 'ANNUITY' in col]\n",
    "print(\"Variables représentant des montants d'argent:\", money_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08fb37c1-92a6-4b3f-98da-13d023e1270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables continues représentant des montants d'argent: ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_YEAR', 'BUREAU_DAYS_CREDIT_min', 'BUREAU_DAYS_CREDIT_mean', 'BUREAU_CREDIT_DAY_OVERDUE_max', 'BUREAU_CREDIT_DAY_OVERDUE_sum', 'BUREAU_AMT_CREDIT_MAX_OVERDUE_mean', 'BUREAU_AMT_CREDIT_MAX_OVERDUE_max', 'BUREAU_AMT_CREDIT_SUM_sum', 'BUREAU_AMT_CREDIT_SUM_DEBT_sum', 'BUREAU_AMT_CREDIT_SUM_OVERDUE_mean', 'BUREAU_AMT_ANNUITY_mean', 'BUREAU_AMT_ANNUITY_sum', 'AMT_BALANCE_sum', 'AMT_CREDIT_LIMIT_ACTUAL_sum', 'AMT_DRAWINGS_ATM_CURRENT_sum', 'AMT_DRAWINGS_CURRENT_sum', 'AMT_PAYMENT_CURRENT_sum', 'AMT_ANNUITY_sum', 'AMT_APPLICATION_sum', 'AMT_CREDIT_sum', 'AMT_DOWN_PAYMENT_sum', 'AMT_GOODS_PRICE_sum', 'INST_AMT_INSTALMENT_sum', 'INST_AMT_PAYMENT_sum', 'BUREAU_YEARS_CREDIT_max', 'BUREAU_YEARS_CREDIT_ENDDATE_max', 'BUREAU_YEARS_CREDIT_UPDATE_mean']\n"
     ]
    }
   ],
   "source": [
    "continues_money_variables = [col for col in money_variables  if col not in finite_vars]\n",
    "print(\"Variables continues représentant des montants d'argent:\", continues_money_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c313377-b9ea-4c67-98bf-ebac6bc5fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def remove_outliers(data, variables_list, num_std=3):\n",
    "    \"\"\"\n",
    "    Supprime les observations avec des valeurs aberrantes des variables spécifiées dans un DataFrame.\n",
    "\n",
    "    :param data: DataFrame contenant les données.\n",
    "    :param variables_list: Liste des noms des variables à nettoyer.\n",
    "    :param num_std: Nombre d'écarts-types utilisés pour définir une valeur aberrante.\n",
    "    :return: DataFrame avec les observations contenant des valeurs aberrantes supprimées.\n",
    "    \"\"\"\n",
    "    for var in variables_list:\n",
    "        if var in data.columns:\n",
    "            # Calculer la moyenne et l'écart-type\n",
    "            mean, std = data[var].mean(), data[var].std()\n",
    "            \n",
    "            # Définir les seuils pour les valeurs aberrantes\n",
    "            lower_bound, upper_bound = mean - num_std * std, mean + num_std * std\n",
    "            \n",
    "            # Filtrer les données pour conserver uniquement les observations sans valeurs aberrantes\n",
    "            data = data[(data[var] > lower_bound) & (data[var] < upper_bound)]\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a11cb74-fb1c-4656-8a9f-29847ecb9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=remove_outliers(train, continues_money_variables, num_std=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08a4b4f4-2fad-488c-a2ce-6132d9c8dbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5048, 318)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd5e11-5317-4bf5-ac0c-97139f412a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_xgboost_with_cross_validation(df, target_column, n_splits=5, random_state=42, eval_metric='logloss'):\n",
    "    \"\"\"\n",
    "    Entraîne un classificateur XGBoost en utilisant la validation croisée et retourne les importances moyennes des caractéristiques.\n",
    "\n",
    "    Paramètres :\n",
    "    - df (pd.DataFrame) : Le dataframe d'entrée contenant les caractéristiques et la cible.\n",
    "    - target_column (str) : Le nom de la colonne cible.\n",
    "    - n_splits (int) : Le nombre de subdivisions pour la validation croisée KFold.\n",
    "    - random_state (int) : La graine utilisée par le générateur de nombres aléatoires.\n",
    "    - eval_metric (str) : La métrique d'évaluation à utiliser pour l'entraînement du modèle.\n",
    "\n",
    "    Retourne :\n",
    "    - pd.DataFrame : Un dataframe avec deux colonnes : Feature et Importance, triées par importance.\n",
    "    \"\"\"\n",
    "\n",
    "    # Séparation des caractéristiques et de la cible\n",
    "    X_train = df.drop(target_column, axis=1)\n",
    "    y_train = df[target_column]\n",
    "\n",
    "    # Configuration de la validation croisée\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Initialisation de la liste pour stocker les importances des caractéristiques\n",
    "    feature_importances = []\n",
    "\n",
    "    # Boucle sur chaque division de la validation croisée\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # Séparation des données en ensembles d'entraînement et de test\n",
    "        X_train_cv, X_test_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Entraînement du modèle\n",
    "        model = xgb.XGBClassifier(random_state=random_state, use_label_encoder=False, eval_metric=eval_metric)\n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "        # Stockage des importances des caractéristiques\n",
    "        feature_importances.append(model.feature_importances_)\n",
    "\n",
    "    # Calcul de la moyenne des importances des caractéristiques sur toutes les divisions\n",
    "    mean_importances = np.mean(feature_importances, axis=0)\n",
    "\n",
    "    # Création d'un DataFrame pour les importances des caractéristiques\n",
    "    importances_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': mean_importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return importances_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d05bf3d-97ad-43e4-a4af-38d1a2981861",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43muu\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'uu' is not defined"
     ]
    }
   ],
   "source": [
    "uu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d3c5f8-6e1d-4329-920a-188ebe7608e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log_distributions(train, vars_to_log_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c8cda-1501-4e55-8cbf-ade940779d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer la transformation logarithmique\n",
    "train = symmetrical_log_transform(train, vars_to_log_transform)\n",
    "test = symmetrical_log_transform(test, vars_to_log_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc7b4d-7c08-48bd-aebb-a88bf61a440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bimodal_vars=['BUREAU_AMT_CREDIT_MAX_OVERDUE_max_log','BUREAU_AMT_CREDIT_SUM_DEBT_sum_log','BUREAU_AMT_ANNUITY_mean_log','BUREAU_AMT_ANNUITY_sum_log','AMT_BALANCE_sum_log', 'AMT_DRAWINGS_ATM_CURRENT_sum_log', 'AMT_PAYMENT_CURRENT_sum_log', 'AMT_DOWN_PAYMENT_sum_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c1bbc4-edb9-43a4-b60c-bf0d87951ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_variables = [col for col in train.columns if 'log' in col]\n",
    "print(\"Variables représentant des montants d'argent:\", transformed_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21edb7f-860e-4802-a63c-bc006bbed41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_list=[var for var in transformed_variables  if var not in bimodal_vars]\n",
    "variables_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1567c8-6152-433a-8014-cb97df63398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_cv = train_xgboost_with_cross_validation(train, 'TARGET')\n",
    "print(importances_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead901f-58e2-48d7-b29f-f7595b9c40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(importances_cv,30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b62701-b466-4724-a19f-2b4461de73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance_histogram(importances_cv['Importance'], num_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e98a14-30b0-446f-95b0-b933e73eb73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def calculate_variable_removal_impact(data, threshold,target='TARGET'):\n",
    "    \"\"\"\n",
    "    Calcule l'impact potentiel de la suppression de variables en fonction d'un seuil d'importance.\n",
    "\n",
    "    Paramètres :\n",
    "    - data (pd.DataFrame) : Le dataframe contenant les données avec les variables à évaluer.\n",
    "    - threshold (float) : Le seuil d'importance en pourcentage en dessous duquel les variables seront supprimées.\n",
    "\n",
    "    Retourne :\n",
    "    - dict : Un dictionnaire contenant les performances du modèle avant et après la suppression de variables.\n",
    "    \"\"\"\n",
    "\n",
    "    # Séparation des caractéristiques et de la cible\n",
    "    X = data.drop(target, axis=1)\n",
    "    y = data[target]\n",
    "\n",
    "    # Création d'un modèle XGBoost initial\n",
    "    xgb_model_initial = xgb.XGBClassifier(random_state=42)\n",
    "    \n",
    "    # Division des données en ensembles d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Entraînement du modèle initial\n",
    "    xgb_model_initial.fit(X_train, y_train)\n",
    "    \n",
    "    # Évaluation du modèle initial\n",
    "    y_pred_initial = xgb_model_initial.predict(X_test)\n",
    "    accuracy_initial = accuracy_score(y_test, y_pred_initial)\n",
    "    roc_auc_initial = roc_auc_score(y_test, y_pred_initial)\n",
    "    \n",
    "    # Calcul de l'importance des caractéristiques\n",
    "    feature_importances = xgb_model_initial.feature_importances_\n",
    "    \n",
    "    # Création d'un masque pour les variables à supprimer\n",
    "    variables_to_remove = feature_importances <= threshold\n",
    "    \n",
    "    # Suppression des variables en dessous du seuil\n",
    "    X_reduced = X.loc[:, ~variables_to_remove]\n",
    "    \n",
    "    # Création d'un modèle XGBoost réduit\n",
    "    xgb_model_reduced = xgb.XGBClassifier(random_state=42)\n",
    "    \n",
    "    # Division des données réduites en ensembles d'entraînement et de test\n",
    "    X_train_reduced, X_test_reduced, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Entraînement du modèle réduit\n",
    "    xgb_model_reduced.fit(X_train_reduced, y_train)\n",
    "    \n",
    "    # Évaluation du modèle réduit\n",
    "    y_pred_reduced = xgb_model_reduced.predict(X_test_reduced)\n",
    "    accuracy_reduced = accuracy_score(y_test, y_pred_reduced)\n",
    "    roc_auc_reduced = roc_auc_score(y_test, y_pred_reduced)\n",
    "    \n",
    "    # Calcul de l'impact potentiel sur les performances\n",
    "    impact_potentiel = {\n",
    "        'accuracy_initial': accuracy_initial,\n",
    "        'roc_auc_initial': roc_auc_initial,\n",
    "        'accuracy_reduced': accuracy_reduced,\n",
    "        'roc_auc_reduced': roc_auc_reduced,\n",
    "        'variables_removed': list(X.columns[variables_to_remove])\n",
    "    }\n",
    "    \n",
    "    return impact_potentiel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b9eeb-c0c7-45db-b873-11fbfbc3205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_variable_removal_impact(train, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cca30f-c8e0-4cae-aa35-ca699a3a931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_variable_removal_impact_cv(data, threshold, num_folds=5, target='TARGET'):\n",
    "    \"\"\"\n",
    "    Évalue l'impact potentiel de la suppression de variables en utilisant la validation croisée.\n",
    "\n",
    "    Paramètres :\n",
    "    - data (pd.DataFrame) : Le dataframe contenant les données avec les variables à évaluer.\n",
    "    - threshold (float) : Le seuil d'importance en pourcentage en dessous duquel les variables seront supprimées.\n",
    "    - num_folds (int) : Le nombre de plis à utiliser pour la validation croisée.\n",
    "    - target (str) : Le nom de la colonne cible.\n",
    "\n",
    "    Retourne :\n",
    "    - dict : Un dictionnaire contenant les performances du modèle avant et après la suppression de variables.\n",
    "    \"\"\"\n",
    "\n",
    "    # Séparation des caractéristiques et de la cible\n",
    "    X = data.drop(target, axis=1)\n",
    "    y = data[target]\n",
    "\n",
    "    # Création d'un modèle XGBoost initial\n",
    "    xgb_model_initial = xgb.XGBClassifier(random_state=42)\n",
    "    \n",
    "    # Division des données en ensembles d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Entraînement du modèle initial\n",
    "    xgb_model_initial.fit(X_train, y_train)\n",
    "    \n",
    "    # Évaluation du modèle initial avec validation croisée\n",
    "    accuracy_initial = np.mean(cross_val_score(xgb_model_initial, X, y, cv=num_folds, scoring='accuracy'))\n",
    "    \n",
    "    # Calcul de l'importance des caractéristiques\n",
    "    feature_importances = xgb_model_initial.feature_importances_\n",
    "    \n",
    "    # Création d'un masque pour les variables à supprimer\n",
    "    variables_to_remove = feature_importances <= threshold\n",
    "    \n",
    "    # Suppression des variables en dessous du seuil\n",
    "    X_reduced = X.loc[:, ~variables_to_remove]\n",
    "    \n",
    "    # Création d'un modèle XGBoost réduit\n",
    "    xgb_model_reduced = xgb.XGBClassifier(random_state=42)\n",
    "    \n",
    "    # Entraînement du modèle réduit\n",
    "    xgb_model_reduced.fit(X_train, y_train)\n",
    "    \n",
    "    # Évaluation du modèle réduit avec validation croisée\n",
    "    accuracy_reduced = np.mean(cross_val_score(xgb_model_reduced, X_reduced, y, cv=num_folds, scoring='accuracy'))\n",
    "    \n",
    "    # Calcul de l'impact potentiel sur les performances\n",
    "    impact_potentiel_acc = {\n",
    "        'accuracy_initial': accuracy_initial,\n",
    "        'accuracy_reduced': accuracy_reduced,\n",
    "        'variables_removed': list(X.columns[variables_to_remove])\n",
    "    }\n",
    "    \n",
    "    return impact_potentiel_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b29edf-3505-44e2-b915-48ed2e9eb7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_variable_removal_impact_cv(train, threshold=0.01, num_folds=5, target='TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f2260-9ddf-4634-b86f-f29edce891ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evaluate_variable_removal_impact_auc(data, threshold, num_folds=5, target='TARGET'):\n",
    "    \"\"\"\n",
    "    Évalue l'impact potentiel de la suppression de variables en utilisant l'aire sous la courbe ROC (AUC-ROC).\n",
    "\n",
    "    Paramètres :\n",
    "    - data (pd.DataFrame) : Le dataframe contenant les données avec les variables à évaluer.\n",
    "    - threshold (float) : Le seuil d'importance en pourcentage en dessous duquel les variables seront supprimées.\n",
    "    - num_folds (int) : Le nombre de plis à utiliser pour la validation croisée.\n",
    "    - target (str) : Le nom de la colonne cible.\n",
    "\n",
    "    Retourne :\n",
    "    - dict : Un dictionnaire contenant les performances du modèle avant et après la suppression de variables.\n",
    "    \"\"\"\n",
    "\n",
    "    # Séparation des caractéristiques et de la cible\n",
    "    X = data.drop(target, axis=1)\n",
    "    y = data[target]\n",
    "\n",
    "    # Création d'un modèle XGBoost initial\n",
    "    xgb_model_initial = xgb.XGBClassifier(random_state=42)\n",
    "    \n",
    "    # Division des données en ensembles d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Entraînement du modèle initial\n",
    "    xgb_model_initial.fit(X_train, y_train)\n",
    "    \n",
    "    # Calcul de l'AUC-ROC du modèle initial\n",
    "    auc_roc_initial = np.mean(cross_val_score(xgb_model_initial, X, y, cv=num_folds, scoring='roc_auc'))\n",
    "    \n",
    "    # Calcul de l'importance des caractéristiques\n",
    "    feature_importances = xgb_model_initial.feature_importances_\n",
    "    \n",
    "    # Création d'un masque pour les variables à supprimer\n",
    "    variables_to_remove = feature_importances <= threshold\n",
    "    \n",
    "    # Suppression des variables en dessous du seuil\n",
    "    X_reduced = X.loc[:, ~variables_to_remove]\n",
    "    \n",
    "    # Création d'un modèle XGBoost réduit\n",
    "    xgb_model_reduced = xgb.XGBClassifier(random_state=42)\n",
    "    \n",
    "    # Entraînement du modèle réduit\n",
    "    xgb_model_reduced.fit(X_train, y_train)\n",
    "    \n",
    "    # Calcul de l'AUC-ROC du modèle réduit\n",
    "    auc_roc_reduced = np.mean(cross_val_score(xgb_model_reduced, X_reduced, y, cv=num_folds, scoring='roc_auc'))\n",
    "    \n",
    "    # Calcul de l'impact potentiel sur les performances\n",
    "    impact_potentiel_auc = {\n",
    "        'auc_roc_initial': auc_roc_initial,\n",
    "        'auc_roc_reduced': auc_roc_reduced,\n",
    "        'variables_removed': list(X.columns[variables_to_remove])\n",
    "    }\n",
    "    \n",
    "    return impact_potentiel_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cdfbf1-5f87-4046-9f63-c92ba3040778",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_potentiel_auc=evaluate_variable_removal_impact_auc(train, threshold=0.008, num_folds=5, target='TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ed989-2a67-4869-8e00-b0aa8de25823",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_potentiel_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c74932a-df95-4b3f-8a87-4fef27f4cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_variables_based_on_impact(data, impact_potential):\n",
    "    \"\"\"\n",
    "    Supprime les variables en fonction de l'impact potentiel calculé.\n",
    "\n",
    "    Paramètres :\n",
    "    - data (pd.DataFrame) : Le DataFrame contenant les données.\n",
    "    - impact_potential (dict) : Un dictionnaire avec les clés 'variables_removed' contenant la liste des variables à supprimer.\n",
    "    - threshold (float) : Le seuil pour décider quelles variables supprimer.\n",
    "\n",
    "    Retourne :\n",
    "    - pd.DataFrame : Le DataFrame de données modifié avec les variables supprimées.\n",
    "    \"\"\"\n",
    "    variables_to_remove = impact_potential.get('variables_removed', [])\n",
    "\n",
    "    # Supprime les variables dont l'impact potentiel est inférieur ou égal au seuil\n",
    "    data_filtered = data.drop(columns=variables_to_remove)\n",
    "\n",
    "    return data_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a191192-9baf-4e62-abe6-61cf136462e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation\n",
    "\n",
    "train_filtered = remove_variables_based_on_impact(train, impact_potentiel_auc)\n",
    "\n",
    "# data_filtered contient maintenant les données avec les variables supprimées en fonction de l'impact potentiel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa3376-6dc5-405e-8190-d84b9c996739",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filtered = remove_variables_based_on_impact(test, impact_potentiel_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22834d26-30a6-465c-bc5b-450020beffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ba5567-de37-4ba9-8db4-dffc6ba09d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize_non_discrete_variables(df, max_unique_values=10):\n",
    "    # Créez une liste de colonnes non discrètes\n",
    "    non_discrete_cols = [col for col in df.columns if len(df[col].unique()) > max_unique_values]\n",
    "    \n",
    "    # Sélectionnez uniquement les colonnes non discrètes\n",
    "    df_non_discrete = df[non_discrete_cols]\n",
    "\n",
    "    # Initialisez le StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Normalisez les colonnes non discrètes\n",
    "    df_normalized = pd.DataFrame(scaler.fit_transform(df_non_discrete), columns=df_non_discrete.columns)\n",
    "\n",
    "    # Remplacez les colonnes originales par les colonnes normalisées dans le DataFrame d'origine\n",
    "    df[non_discrete_cols] = df_normalized\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe90f05-1d52-46ee-ba78-04ec67f827d4",
   "metadata": {},
   "source": [
    "# Exemple d'utilisation avec un DataFrame appelé train_numerized\n",
    "train= normalize_non_discrete_variables(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4ef6fb-cbb4-4bcc-9222-43feeb229f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train_numerized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c4ade1-1918-4587-901a-5b62310c5012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "def train_xgboost_and_get_feature_importance(df, target_column, random_state=42, eval_metric='logloss'):\n",
    "   \n",
    "    \"\"\"\n",
    "    Entraîne un classificateur XGBoost sur le dataframe donné et la colonne cible,\n",
    "    puis retourne un dataframe avec l'importance des caractéristiques.\n",
    "\n",
    "    Paramètres :\n",
    "    - df (pd.DataFrame) : Le dataframe d'entrée contenant les caractéristiques et la cible.\n",
    "    - target_column (str) : Le nom de la colonne cible.\n",
    "    - random_state (int) : La graine utilisée par le générateur de nombres aléatoires.\n",
    "    - eval_metric (str) : La métrique d'évaluation à utiliser pour l'entraînement du modèle.\n",
    "\n",
    "    Retourne :\n",
    "    - pd.DataFrame : Un dataframe avec deux colonnes : Feature et Importance, triées par importance.\n",
    "    \"\"\"\n",
    "\n",
    "    # Séparation des caractéristiques et de la cible\n",
    "    X_train = df.drop(target_column, axis=1)\n",
    "    y_train = df[target_column]\n",
    "\n",
    "    # Création et entraînement du modèle XGBoost\n",
    "    xgb_model = xgb.XGBClassifier(random_state=random_state, use_label_encoder=False, eval_metric=eval_metric)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Obtention de l'importance des caractéristiques\n",
    "    feature_importances = xgb_model.feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "    importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "\n",
    "    # Retourner le DataFrame des importances triées\n",
    "    return importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Utilisation de la fonction :\n",
    "# Remplacez 'your_dataframe' par votre DataFrame réel et 'your_target_column' par le nom réel de votre colonne cible.\n",
    "importances = train_xgboost_and_get_feature_importance(train, 'TARGET')\n",
    "print(importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff93489d-8408-45ec-a9f4-fff44bbecd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_importance_histogram(importance_values, num_bins=10):\n",
    "    # Convertir en array numpy si ce n'est pas déjà le cas\n",
    "    importance_values = np.array(importance_values)\n",
    "    \n",
    "    # Créer les bins adaptés aux valeurs d'importance\n",
    "    max_importance = np.max(importance_values)\n",
    "    bins = np.linspace(0, max_importance, num_bins + 1)\n",
    "    \n",
    "    # Utiliser np.histogram pour obtenir le compte des valeurs dans chaque bin\n",
    "    counts, edges = np.histogram(importance_values, bins)\n",
    "    \n",
    "    # Plot the histogram\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.bar(range(num_bins), counts, width=1, align='center', edgecolor='k')\n",
    "    plt.xticks(range(num_bins), [f'{edges[i]:.4f}-{edges[i+1]:.4f}' for i in range(num_bins)])\n",
    "    plt.xlabel('Importance Intervals')\n",
    "    plt.ylabel('Number of Variables')\n",
    "    plt.title('Number of Variables per Importance Interval')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b2925-43af-490f-8355-610c9a5e849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation de la fonction avec vos valeurs d'importance\n",
    "plot_importance_histogram(importances['Importance'], num_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4518783c-f958-4f76-9d04-ff4e6bd25e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_feature_importance(importances, num_features=50):\n",
    "    # Tri des importances par ordre décroissant\n",
    "    importances_sorted = importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Sélectionner les meilleures caractéristiques\n",
    "    top_features = importances_sorted.head(num_features)\n",
    "\n",
    "    # Création du diagramme de barres horizontales\n",
    "    plt.figure(figsize=(10, 6))  # Ajustez la taille de la figure si nécessaire\n",
    "    plt.barh(top_features['Feature'], top_features['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title(f'Importance des {num_features} Caractéristiques les plus importantes')\n",
    "    plt.gca().invert_yaxis()  # Inverser l'axe y pour afficher les caractéristiques les plus importantes en haut\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097dc9e2-cab6-45b5-8019-18384d207d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation de la méthode avec le DataFrame des importances et spécification du nombre de caractéristiques à afficher\n",
    "plot_feature_importance(importances, num_features=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d59dd9-e9ed-444f-80b8-6121d2514348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_xgboost_with_cross_validation(df, target_column, n_splits=5, random_state=42, eval_metric='logloss'):\n",
    "    \"\"\"\n",
    "    Entraîne un classificateur XGBoost en utilisant la validation croisée et retourne les importances moyennes des caractéristiques.\n",
    "\n",
    "    Paramètres :\n",
    "    - df (pd.DataFrame) : Le dataframe d'entrée contenant les caractéristiques et la cible.\n",
    "    - target_column (str) : Le nom de la colonne cible.\n",
    "    - n_splits (int) : Le nombre de subdivisions pour la validation croisée KFold.\n",
    "    - random_state (int) : La graine utilisée par le générateur de nombres aléatoires.\n",
    "    - eval_metric (str) : La métrique d'évaluation à utiliser pour l'entraînement du modèle.\n",
    "\n",
    "    Retourne :\n",
    "    - pd.DataFrame : Un dataframe avec deux colonnes : Feature et Importance, triées par importance.\n",
    "    \"\"\"\n",
    "\n",
    "    # Séparation des caractéristiques et de la cible\n",
    "    X_train = df.drop(target_column, axis=1)\n",
    "    y_train = df[target_column]\n",
    "\n",
    "    # Configuration de la validation croisée\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Initialisation de la liste pour stocker les importances des caractéristiques\n",
    "    feature_importances = []\n",
    "\n",
    "    # Boucle sur chaque division de la validation croisée\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # Séparation des données en ensembles d'entraînement et de test\n",
    "        X_train_cv, X_test_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Entraînement du modèle\n",
    "        model = xgb.XGBClassifier(random_state=random_state, use_label_encoder=False, eval_metric=eval_metric)\n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "        # Stockage des importances des caractéristiques\n",
    "        feature_importances.append(model.feature_importances_)\n",
    "\n",
    "    # Calcul de la moyenne des importances des caractéristiques sur toutes les divisions\n",
    "    mean_importances = np.mean(feature_importances, axis=0)\n",
    "\n",
    "    # Création d'un DataFrame pour les importances des caractéristiques\n",
    "    importances_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': mean_importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return importances_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c134be54-1e72-484b-ac93-22bc30d0974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "class TestXGBoostCV(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # Charger le jeu de données Iris\n",
    "        iris = load_iris()\n",
    "        self.X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "        self.y = iris.target\n",
    "        self.df = self.X.copy()\n",
    "        self.df['target'] = self.y\n",
    "\n",
    "    def test_kfold_splits(self):\n",
    "        n_splits = 5\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        splits = list(kf.split(self.df.drop('target', axis=1)))\n",
    "        self.assertEqual(len(splits), n_splits)\n",
    "\n",
    "    def test_feature_importances_shape(self):\n",
    "        df_importances = train_xgboost_with_cross_validation(self.df, 'target')\n",
    "        self.assertEqual(df_importances.shape[1], 2)  # Deux colonnes : Feature et Importance\n",
    "\n",
    "    def test_feature_importances_content(self):\n",
    "        df_importances = train_xgboost_with_cross_validation(self.df, 'target')\n",
    "        self.assertTrue(all(df_importances['Importance'] >= 0))\n",
    "\n",
    "    def test_model_training(self):\n",
    "        df_importances = train_xgboost_with_cross_validation(self.df, 'target')\n",
    "        self.assertFalse(df_importances.empty)\n",
    "\n",
    "    def test_fold_importances(self):\n",
    "        n_splits = 5\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        X = self.df.drop('target', axis=1)\n",
    "        y = self.df['target']\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train_cv = X.iloc[train_index]\n",
    "            y_train_cv = y.iloc[train_index]\n",
    "\n",
    "            model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "            model.fit(X_train_cv, y_train_cv)\n",
    "            self.assertTrue(np.all(model.feature_importances_ >= 0))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802a6814-bd6e-420b-ba78-86a38b655f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "class TestXGBoostCV(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    Ce script de test unitaire vérifie plusieurs aspects de notre fonction de validation croisée :\n",
    "    \n",
    "    1. Le nombre correct de splits est créé par KFold.\n",
    "    2. Le DataFrame des importances des caractéristiques a la bonne forme.\n",
    "    3. Les valeurs des importances sont des nombres réels non négatifs.\n",
    "    4. Le modèle est bien entraîné et renvoie des importances des caractéristiques.\n",
    "    5. Chaque fold renvoie des importances des caractéristiques non nulles et positives.  \n",
    "    \"\"\"\n",
    "    def setUp(self):\n",
    "        # Créer un jeu de données de test\n",
    "        self.X, self.y = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, random_state=42)\n",
    "        self.df = pd.DataFrame(self.X)\n",
    "        self.df['target'] = self.y\n",
    "\n",
    "    def test_kfold_splits(self):\n",
    "        # Testez si KFold crée le bon nombre de splits\n",
    "        n_splits = 5\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        splits = list(kf.split(self.df.drop('target', axis=1)))\n",
    "        self.assertEqual(len(splits), n_splits)\n",
    "\n",
    "    def test_feature_importances_shape(self):\n",
    "        # Testez si le DataFrame des importances des caractéristiques a la bonne forme\n",
    "        df_importances = train_xgboost_with_cross_validation(self.df, 'target')\n",
    "        self.assertEqual(df_importances.shape[1], 2) # Deux colonnes : Feature et Importance\n",
    "\n",
    "    def test_feature_importances_content(self):\n",
    "        # Testez si les importances des caractéristiques sont des nombres réels non négatifs\n",
    "        df_importances = train_xgboost_with_cross_validation(self.df, 'target')\n",
    "        self.assertTrue(all(df_importances['Importance'] >= 0))\n",
    "    \n",
    "    def test_model_training(self):\n",
    "        # Testez si le modèle est bien entraîné et renvoie un objet\n",
    "        df_importances = train_xgboost_with_cross_validation(self.df, 'target')\n",
    "        self.assertFalse(df_importances.empty)\n",
    "    \n",
    "    def test_fold_importances(self):\n",
    "        # Testez si l'importance des caractéristiques pour chaque fold est enregistrée et non nulle\n",
    "        n_splits = 5\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        X_train = self.df.drop('target', axis=1)\n",
    "        y_train = self.df['target']\n",
    "        feature_importances = []\n",
    "        for train_index, _ in kf.split(X_train):\n",
    "            X_train_cv = X_train.iloc[train_index]\n",
    "            y_train_cv = y_train.iloc[train_index]\n",
    "    \n",
    "            model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "            model.fit(X_train_cv, y_train_cv)\n",
    "            feature_importances.append(model.feature_importances_)\n",
    "\n",
    "        # Assurez-vous qu'il y a des importances enregistrées pour chaque fold\n",
    "        self.assertEqual(len(feature_importances), n_splits)\n",
    "        # Vérifiez que chaque liste d'importance n'est pas vide\n",
    "        for importance in feature_importances:\n",
    "            self.assertTrue(len(importance) > 0)\n",
    "            self.assertTrue(np.all(importance >= 0))\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        unittest.main(argv=[''], exit=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e98ea7d-64bb-49b4-ac3e-da69f45deeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_cv = train_xgboost_with_cross_validation(train, 'TARGET')\n",
    "print(importances_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3783fb46-b22f-45e8-ad08-22973a91f45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(importances_cv,30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99c696b-d7a2-4ac3-b32c-176d0a3833e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance_histogram(importances_cv['Importance'], num_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f9df3a-8b62-4fa6-bd1f-5a438ffb0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def calculate_variable_removal_impact(data, threshold,target='TARGET'):\n",
    "    \"\"\"\n",
    "    Calcule l'impact potentiel de la suppression de variables en fonction d'un seuil d'importance.\n",
    "\n",
    "    Paramètres :\n",
    "    - data (pd.DataFrame) : Le dataframe contenant les données avec les variables à évaluer.\n",
    "    - threshold (float) : Le seuil d'importance en pourcentage en dessous duquel les variables seront supprimées.\n",
    "\n",
    "    Retourne :\n",
    "    - dict : Un dictionnaire contenant les performances du modèle avant et après la suppression de variables.\n",
    "    \"\"\"\n",
    "\n",
    "    # Séparation des caractéristiques et de la cible\n",
    "    X = data.drop(target, axis=1)\n",
    "    y = data[target]\n",
    "\n",
    "    # Création d'un modèle XGBoost initial\n",
    "    xgb_model_initial = xgb.XGBClassifier(random_state=42)\n",
    "    \n",
    "    # Division des données en ensembles d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Entraînement du modèle initial\n",
    "    xgb_model_initial.fit(X_train, y_train)\n",
    "    \n",
    "    # Évaluation du modèle initial\n",
    "    y_pred_initial = xgb_model_initial.predict(X_test)\n",
    "    accuracy_initial = accuracy_score(y_test, y_pred_initial)\n",
    "    roc_auc_initial = roc_auc_score(y_test, y_pred_initial)\n",
    "    \n",
    "    # Calcul de l'importance des caractéristiques\n",
    "    feature_importances = xgb_model_initial.feature_importances_\n",
    "    \n",
    "    # Création d'un masque pour les variables à supprimer\n",
    "    variables_to_remove = feature_importances <= threshold\n",
    "    \n",
    "    # Suppression des variables en dessous du seuil\n",
    "    X_reduced = X.loc[:, ~variables_to_remove]\n",
    "    \n",
    "    # Création d'un modèle XGBoost réduit\n",
    "    xgb_model_reduced = xgb.XGBClassifier(random_state=42)\n",
    "    \n",
    "    # Division des données réduites en ensembles d'entraînement et de test\n",
    "    X_train_reduced, X_test_reduced, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Entraînement du modèle réduit\n",
    "    xgb_model_reduced.fit(X_train_reduced, y_train)\n",
    "    \n",
    "    # Évaluation du modèle réduit\n",
    "    y_pred_reduced = xgb_model_reduced.predict(X_test_reduced)\n",
    "    accuracy_reduced = accuracy_score(y_test, y_pred_reduced)\n",
    "    roc_auc_reduced = roc_auc_score(y_test, y_pred_reduced)\n",
    "    \n",
    "    # Calcul de l'impact potentiel sur les performances\n",
    "    impact_potentiel = {\n",
    "        'accuracy_initial': accuracy_initial,\n",
    "        'roc_auc_initial': roc_auc_initial,\n",
    "        'accuracy_reduced': accuracy_reduced,\n",
    "        'roc_auc_reduced': roc_auc_reduced,\n",
    "        'variables_removed': list(X.columns[variables_to_remove])\n",
    "    }\n",
    "    \n",
    "    return impact_potentiel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d32298-57f6-4339-aac6-519814173233",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_variable_removal_impact(train, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1521ee9-c6a9-486d-be81-2d4d3db9c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_variable_removal_impact_cv(data, threshold, num_folds=5, target='TARGET'):\n",
    "    \"\"\"\n",
    "    Évalue l'impact potentiel de la suppression de variables en utilisant la validation croisée.\n",
    "\n",
    "    Paramètres :\n",
    "    - data (pd.DataFrame) : Le dataframe contenant les données avec les variables à évaluer.\n",
    "    - threshold (float) : Le seuil d'importance en pourcentage en dessous duquel les variables seront supprimées.\n",
    "    - num_folds (int) : Le nombre de plis à utiliser pour la validation croisée.\n",
    "    - target (str) : Le nom de la colonne cible.\n",
    "\n",
    "    Retourne :\n",
    "    - dict : Un dictionnaire contenant les performances du modèle avant et après la suppression de variables.\n",
    "    \"\"\"\n",
    "\n",
    "    # Séparation des caractéristiques et de la cible\n",
    "    X = data.drop(target, axis=1)\n",
    "    y = data[target]\n",
    "\n",
    "    # Création d'un modèle XGBoost initial\n",
    "    xgb_model_initial = xgb.XGBClassifier(random_state=42)\n",
    "    \n",
    "    # Division des données en ensembles d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Entraînement du modèle initial\n",
    "    xgb_model_initial.fit(X_train, y_train)\n",
    "    \n",
    "    # Évaluation du modèle initial avec validation croisée\n",
    "    accuracy_initial = np.mean(cross_val_score(xgb_model_initial, X, y, cv=num_folds, scoring='accuracy'))\n",
    "    \n",
    "    # Calcul de l'importance des caractéristiques\n",
    "    feature_importances = xgb_model_initial.feature_importances_\n",
    "    \n",
    "    # Création d'un masque pour les variables à supprimer\n",
    "    variables_to_remove = feature_importances <= threshold\n",
    "    \n",
    "    # Suppression des variables en dessous du seuil\n",
    "    X_reduced = X.loc[:, ~variables_to_remove]\n",
    "    \n",
    "    # Création d'un modèle XGBoost réduit\n",
    "    xgb_model_reduced = xgb.XGBClassifier(random_state=42)\n",
    "    \n",
    "    # Entraînement du modèle réduit\n",
    "    xgb_model_reduced.fit(X_train, y_train)\n",
    "    \n",
    "    # Évaluation du modèle réduit avec validation croisée\n",
    "    accuracy_reduced = np.mean(cross_val_score(xgb_model_reduced, X_reduced, y, cv=num_folds, scoring='accuracy'))\n",
    "    \n",
    "    # Calcul de l'impact potentiel sur les performances\n",
    "    impact_potentiel_acc = {\n",
    "        'accuracy_initial': accuracy_initial,\n",
    "        'accuracy_reduced': accuracy_reduced,\n",
    "        'variables_removed': list(X.columns[variables_to_remove])\n",
    "    }\n",
    "    \n",
    "    return impact_potentiel_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcaa491-6599-4dd2-8e19-a34931ce7423",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_variable_removal_impact_cv(train, threshold=0.01, num_folds=5, target='TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466943c-cf4c-4e6a-8abb-a2fedf8734fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evaluate_variable_removal_impact_auc(data, threshold, num_folds=5, target='TARGET'):\n",
    "    \"\"\"\n",
    "    Évalue l'impact potentiel de la suppression de variables en utilisant l'aire sous la courbe ROC (AUC-ROC).\n",
    "\n",
    "    Paramètres :\n",
    "    - data (pd.DataFrame) : Le dataframe contenant les données avec les variables à évaluer.\n",
    "    - threshold (float) : Le seuil d'importance en pourcentage en dessous duquel les variables seront supprimées.\n",
    "    - num_folds (int) : Le nombre de plis à utiliser pour la validation croisée.\n",
    "    - target (str) : Le nom de la colonne cible.\n",
    "\n",
    "    Retourne :\n",
    "    - dict : Un dictionnaire contenant les performances du modèle avant et après la suppression de variables.\n",
    "    \"\"\"\n",
    "\n",
    "    # Séparation des caractéristiques et de la cible\n",
    "    X = data.drop(target, axis=1)\n",
    "    y = data[target]\n",
    "\n",
    "    # Création d'un modèle XGBoost initial\n",
    "    xgb_model_initial = xgb.XGBClassifier(random_state=42)\n",
    "    \n",
    "    # Division des données en ensembles d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Entraînement du modèle initial\n",
    "    xgb_model_initial.fit(X_train, y_train)\n",
    "    \n",
    "    # Calcul de l'AUC-ROC du modèle initial\n",
    "    auc_roc_initial = np.mean(cross_val_score(xgb_model_initial, X, y, cv=num_folds, scoring='roc_auc'))\n",
    "    \n",
    "    # Calcul de l'importance des caractéristiques\n",
    "    feature_importances = xgb_model_initial.feature_importances_\n",
    "    \n",
    "    # Création d'un masque pour les variables à supprimer\n",
    "    variables_to_remove = feature_importances <= threshold\n",
    "    \n",
    "    # Suppression des variables en dessous du seuil\n",
    "    X_reduced = X.loc[:, ~variables_to_remove]\n",
    "    \n",
    "    # Création d'un modèle XGBoost réduit\n",
    "    xgb_model_reduced = xgb.XGBClassifier(random_state=42)\n",
    "    \n",
    "    # Entraînement du modèle réduit\n",
    "    xgb_model_reduced.fit(X_train, y_train)\n",
    "    \n",
    "    # Calcul de l'AUC-ROC du modèle réduit\n",
    "    auc_roc_reduced = np.mean(cross_val_score(xgb_model_reduced, X_reduced, y, cv=num_folds, scoring='roc_auc'))\n",
    "    \n",
    "    # Calcul de l'impact potentiel sur les performances\n",
    "    impact_potentiel_auc = {\n",
    "        'auc_roc_initial': auc_roc_initial,\n",
    "        'auc_roc_reduced': auc_roc_reduced,\n",
    "        'variables_removed': list(X.columns[variables_to_remove])\n",
    "    }\n",
    "    \n",
    "    return impact_potentiel_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12578589-3c95-49d8-8ec2-f5bc536f7956",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_potentiel_auc=evaluate_variable_removal_impact_auc(train, threshold=0.008, num_folds=5, target='TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6ad9e0-2de3-422d-affc-0adf783b379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_potentiel_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b89e43-8162-442d-b7f1-dda9e4262881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_variables_based_on_impact(data, impact_potential):\n",
    "    \"\"\"\n",
    "    Supprime les variables en fonction de l'impact potentiel calculé.\n",
    "\n",
    "    Paramètres :\n",
    "    - data (pd.DataFrame) : Le DataFrame contenant les données.\n",
    "    - impact_potential (dict) : Un dictionnaire avec les clés 'variables_removed' contenant la liste des variables à supprimer.\n",
    "    - threshold (float) : Le seuil pour décider quelles variables supprimer.\n",
    "\n",
    "    Retourne :\n",
    "    - pd.DataFrame : Le DataFrame de données modifié avec les variables supprimées.\n",
    "    \"\"\"\n",
    "    variables_to_remove = impact_potential.get('variables_removed', [])\n",
    "\n",
    "    # Supprime les variables dont l'impact potentiel est inférieur ou égal au seuil\n",
    "    data_filtered = data.drop(columns=variables_to_remove)\n",
    "\n",
    "    return data_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbd671c-9989-4b39-9909-1a72ccb7af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation\n",
    "\n",
    "train_filtered = remove_variables_based_on_impact(train, impact_potentiel_auc)\n",
    "\n",
    "# data_filtered contient maintenant les données avec les variables supprimées en fonction de l'impact potentiel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a23a5f-2155-4536-ae33-a709552994e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filtered = remove_variables_based_on_impact(test, impact_potentiel_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b125dffc-2ef7-4429-8501-c39b3a66c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "presence_rate(test_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f243f8c6-ddd6-49f4-b3f1-f2144dfc798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_distributions(df, variables):\n",
    "    # Calculez le nombre de lignes et de colonnes pour organiser les sous-graphes\n",
    "    num_variables = len(variables)\n",
    "    num_cols = int(num_variables ** 0.5)\n",
    "    num_rows = (num_variables + num_cols - 1) // num_cols\n",
    "\n",
    "    # Créez une figure et une grille de sous-graphes\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 10))\n",
    "    fig.subplots_adjust(hspace=1)\n",
    "    \n",
    "    # Parcourez les variables et créez des sous-graphes pour chaque variable\n",
    "    for i, variable in enumerate(variables):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Utilisez Seaborn pour tracer la distribution de la variable\n",
    "        sns.histplot(df[variable], ax=ax, kde=True)\n",
    "        #ax.set_title(variable)\n",
    "        \n",
    "    # Réduisez la taille de la police pour les étiquettes des axes x et y\n",
    "        ax.set_xlabel(ax.get_xlabel(), fontsize=8)\n",
    "        ax.set_ylabel(ax.get_ylabel(), fontsize=8)\n",
    "    \n",
    "    # Supprimez les sous-graphes non utilisés\n",
    "    for i in range(num_variables, num_rows * num_cols):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889fcdda-b603-4e59-b8d1-b211e20026e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_log_distributions(df, monetary_vars, small_value=1e-6):\n",
    "    # Créez une copie temporaire de df pour les transformations\n",
    "    temp_df = df.copy()\n",
    "    \n",
    "    # Liste pour stocker les noms des nouvelles variables transformées\n",
    "    transformed_vars = []\n",
    "    \n",
    "    # Appliquer une transformation symétrique du logarithme\n",
    "    for var in monetary_vars:\n",
    "        if var in temp_df.columns:\n",
    "            transformed_var = var + '_log'\n",
    "            temp_df[transformed_var] = np.sign(temp_df[var]) * np.log(np.abs(temp_df[var]) + small_value)\n",
    "            transformed_vars.append(transformed_var)\n",
    "    \n",
    "    # Calculez le nombre de lignes et de colonnes pour organiser les sous-graphes\n",
    "    num_variables = len(transformed_vars)\n",
    "    num_cols = int(np.ceil(np.sqrt(num_variables)))\n",
    "    num_rows = int(np.ceil(num_variables / num_cols))\n",
    "    \n",
    "    # Créez une figure et une grille de sous-graphes\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 10))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "    \n",
    "    # Parcourez les variables transformées et créez des sous-graphes pour chaque variable\n",
    "    for i, transformed_var in enumerate(transformed_vars):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        ax = axes[row][col] if num_rows > 1 else axes[col]\n",
    "        \n",
    "        # Utilisez Seaborn pour tracer la distribution de la variable\n",
    "        sns.histplot(temp_df[transformed_var], ax=ax, kde=True)\n",
    "        ax.set_xlabel(transformed_var, fontsize=10)\n",
    "        ax.set_ylabel('Count', fontsize=10)\n",
    "    \n",
    "    # Supprimez les sous-graphes non utilisés\n",
    "    for j in range(i+1, num_rows*num_cols):\n",
    "        fig.delaxes(axes.flatten()[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c1e6e-0f1b-406b-a351-86bf4aa0c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "finite_vars=finite_unique_values_variables(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25c8843-6c53-4aec-b8e5-6896fae2ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "money_variables = [col for col in train.columns if 'AMT' in col or 'INCOME' in col or 'CREDIT' in col or 'ANNUITY' in col]\n",
    "print(\"Variables représentant des montants d'argent:\", money_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c257c6f1-a828-4541-a208-3b576775dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "continues_money_variables = [col for col in money_variables  if col not in finite_vars]\n",
    "print(\"Variables continues représentant des montants d'argent:\", continues_money_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157f1c89-91e5-4378-9263-cd7f69b00843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_distributions(train, continues_money_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa572c1-c9c6-4c48-b96e-090ba8ab251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filtered.to_csv('train_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db78c42c-146e-4f94-8b5d-0e3e95926e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filtered.to_csv('test_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae8bdc-6bd9-45f5-a893-edf8729b98b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b98079-0689-4ce3-83b5-02f351880f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "finite_unique_values_variables(train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e1acfb-97b5-42a7-be59-3e7e5779acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filtered.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac169d12-1a3c-4b4d-978a-b4d4530e2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def impute_data(data):\n",
    "    # Créer un dictionnaire pour stocker le nombre de valeurs uniques pour chaque variable\n",
    "    unique_counts = {col: data[col].nunique() for col in data.columns}\n",
    "    \n",
    "    # Filtrer pour obtenir les variables avec un nombre fini de valeurs uniques\n",
    "    # Vous pouvez définir un seuil spécifique si nécessaire, par exemple, moins de 20 valeurs uniques\n",
    "    finite_vars = {col: count for col, count in unique_counts.items() if count < 20}  # Exemple de seuil: 20\n",
    "    \n",
    "    # Liste des variables numériques (à l'exclusion de la cible)\n",
    "    numeric_vars = data.select_dtypes(include='number').columns.tolist()\n",
    "    \n",
    "    # Liste des variables catégorielles (y compris les binaires)\n",
    "    categorical_vars = [col for col in data.columns if col not in numeric_vars]\n",
    "    \n",
    "    # Imputer les valeurs manquantes pour les variables sélectionnées avec le mode\n",
    "    mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    data[list(finite_vars.keys())] = mode_imputer.fit_transform(data[list(finite_vars.keys())])\n",
    "    \n",
    "    # Imputer les valeurs manquantes pour les variables numériques avec la moyenne\n",
    "    mean_imputer = SimpleImputer(strategy='mean')\n",
    "    data[numeric_vars] = mean_imputer.fit_transform(data[numeric_vars])\n",
    "    \n",
    "    # Imputer les valeurs manquantes pour les variables catégorielles avec une nouvelle catégorie 'Missing'\n",
    "    data[categorical_vars] = data[categorical_vars].fillna('Missing')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d932c3-1456-4f2a-b6a7-4aef670764b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer l'imputation à votre ensemble de données train_filtered\n",
    "train_filtered_imputed = impute_data(train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09623a34-0f75-4131-875a-8e9a640044c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filtered_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd955155-48d6-4304-ac12-015bbb48c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer l'imputation à votre ensemble de données train_filtered\n",
    "test_filtered_imputed = impute_data(test_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6813aab0-c6cc-43cb-81ee-953951826927",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filtered_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69912d98-5461-4623-b663-dcf2d387f458",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filtered_imputed.to_csv(\"train_filtered_imputed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fa68ad-e1e1-4e3c-9464-86247dc548ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filtered_imputed.to_csv(\"test_filtered_imputed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0423c00-13b3-4a02-9e88-17b60b650390",
   "metadata": {},
   "outputs": [],
   "source": [
    "presence_rate(train_filtered_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35146e2-b0da-48de-81dc-1d6fcc6433a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_variable_removal_impact_auc(train_filtered_imputed, threshold=0.01, num_folds=5, target='TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b2cbd-826a-45a6-b537-52896633d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e76bb-dde8-4d08-b1f0-1a5124213aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Créez une nouvelle expérience avec un nom spécifique et obtenez son ID\n",
    "def create_or_get_experiment(experiment_name):\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(experiment_name)\n",
    "    else:\n",
    "        experiment_id = experiment.experiment_id\n",
    "    return experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b9111-c536-4423-a65e-27f76f504efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_feature_importance(experiment_id, df, target_column, random_state=42, eval_metric='logloss'):\n",
    "    # Séparation des caractéristiques et de la cible\n",
    "    X_train = df.drop(target_column, axis=1)\n",
    "    y_train = df[target_column]\n",
    "\n",
    "    # Enregistrement d'un nouveau run avec l'ID de l'expérience spécifiée\n",
    "    with mlflow.start_run(experiment_id=experiment_id):\n",
    "        # Entraînement du modèle XGBoost\n",
    "        xgb_model = xgb.XGBClassifier(random_state=random_state, use_label_encoder=False, eval_metric=eval_metric)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "\n",
    "        # Enregistrement des caractéristiques d'importance avec MLflow\n",
    "        mlflow.sklearn.log_model(xgb_model, \"xgboost_model\")\n",
    "        feature_importances = pd.DataFrame({'Feature': X_train.columns, 'Importance': xgb_model.feature_importances_})\n",
    "        \n",
    "        # Enregistrez la DataFrame en tant que fichier CSV temporaire\n",
    "        feature_importances.to_csv(\"feature_importance_temp.csv\", index=False)\n",
    "        \n",
    "        # Enregistrez le fichier CSV temporaire en tant qu'artefact\n",
    "        mlflow.log_artifact(\"feature_importance_temp.csv\", \"feature_importance.csv\")\n",
    "        \n",
    "        # Supprimez le fichier CSV temporaire après l'avoir enregistré en tant qu'artefact\n",
    "        os.remove(\"feature_importance_temp.csv\")\n",
    "\n",
    "        mlflow.log_param(\"eval_metric\", eval_metric)\n",
    "        mlflow.log_param(\"random_state\", random_state)\n",
    "\n",
    "    # Affichage des caractéristiques d'importance\n",
    "    print(\"Caractéristiques d'importance enregistrées avec succès.\")\n",
    "    return feature_importances\n",
    "\n",
    "target_column_name = 'TARGET'\n",
    "experiment_name = \"train_and_log_feature_importance\"\n",
    "\n",
    "# Créez ou obtenez l'ID de l'expérience\n",
    "experiment_id = create_or_get_experiment(experiment_name)\n",
    "\n",
    "# Utilisation de la fonction pour enregistrer les caractéristiques d'importance\n",
    "importances = log_feature_importance(experiment_id, train, target_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5990329e-a9da-49d6-aec1-41739a6b1ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_feature_importance(experiment_id, df, target_column, random_state=42, eval_metric='logloss'):\n",
    "    # Vérifiez si une exécution est déjà active et fermez-la si nécessaire\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()\n",
    "    \n",
    "    # Séparation des caractéristiques et de la cible\n",
    "    X_train = df.drop(target_column, axis=1)\n",
    "    y_train = df[target_column]\n",
    "\n",
    "    # Enregistrement d'un nouveau run avec l'ID de l'expérience spécifiée\n",
    "    with mlflow.start_run(experiment_id=experiment_id):\n",
    "        # Entraînement du modèle XGBoost\n",
    "        xgb_model = xgb.XGBClassifier(random_state=random_state, use_label_encoder=False, eval_metric=eval_metric)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "\n",
    "        # Enregistrement des caractéristiques d'importance avec MLflow\n",
    "        mlflow.sklearn.log_model(xgb_model, \"xgboost_model\")\n",
    "        feature_importances = pd.DataFrame({'Feature': X_train.columns, 'Importance': xgb_model.feature_importances_})\n",
    "        \n",
    "        # Enregistrez la DataFrame en tant que fichier CSV temporaire\n",
    "        feature_importances.to_csv(\"feature_importance_temp.csv\", index=False)\n",
    "        \n",
    "        # Enregistrez le fichier CSV temporaire en tant qu'artefact\n",
    "        mlflow.log_artifact(\"feature_importance_temp.csv\", \"feature_importance.csv\")\n",
    "        \n",
    "        # Supprimez le fichier CSV temporaire après l'avoir enregistré en tant qu'artefact\n",
    "        os.remove(\"feature_importance_temp.csv\")\n",
    "\n",
    "        mlflow.log_param(\"eval_metric\", eval_metric)\n",
    "        mlflow.log_param(\"random_state\", random_state)\n",
    "\n",
    "    # Affichage des caractéristiques d'importance\n",
    "    print(\"Caractéristiques d'importance enregistrées avec succès.\")\n",
    "    return feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018511f-b268-4cf9-9fd4-25262f00543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Créez une nouvelle expérience avec un nom spécifique et obtenez son ID\n",
    "def create_or_get_experiment(experiment_name):\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(experiment_name)\n",
    "    else:\n",
    "        experiment_id = experiment.experiment_id\n",
    "    return experiment_id\n",
    "\n",
    "def log_feature_importance(experiment_id, df, target_column, random_state=42, eval_metric='logloss'):\n",
    "    # Séparation des caractéristiques et de la cible\n",
    "    X_train = df.drop(target_column, axis=1)\n",
    "    y_train = df[target_column]\n",
    "\n",
    "    # Enregistrement d'un nouveau run avec l'ID de l'expérience spécifiée\n",
    "    with mlflow.start_run(experiment_id=experiment_id, nested=True):\n",
    "        # Entraînement du modèle XGBoost\n",
    "        xgb_model = xgb.XGBClassifier(random_state=random_state, use_label_encoder=False, eval_metric=eval_metric)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "\n",
    "        # Enregistrement des caractéristiques d'importance avec MLflow\n",
    "        mlflow.sklearn.log_model(xgb_model, \"xgboost_model\")\n",
    "        feature_importances = pd.DataFrame({'Feature': X_train.columns, 'Importance': xgb_model.feature_importances_})\n",
    "        \n",
    "        # Enregistrez la DataFrame en tant que fichier CSV temporaire\n",
    "        feature_importances.to_csv(\"feature_importance_temp.csv\", index=False)\n",
    "        \n",
    "        # Enregistrez le fichier CSV temporaire en tant qu'artefact\n",
    "        mlflow.log_artifact(\"feature_importance_temp.csv\", \"feature_importance.csv\")\n",
    "        \n",
    "        # Supprimez le fichier CSV temporaire après l'avoir enregistré en tant qu'artefact\n",
    "        os.remove(\"feature_importance_temp.csv\")\n",
    "\n",
    "        mlflow.log_param(\"eval_metric\", eval_metric)\n",
    "        mlflow.log_param(\"random_state\", random_state)\n",
    "\n",
    "    # Affichage des caractéristiques d'importance\n",
    "    print(\"Caractéristiques d'importance enregistrées avec succès.\")\n",
    "    return feature_importances\n",
    "\n",
    "target_column_name = 'TARGET'\n",
    "experiment_name = \"train_and_log_feature_importance\"\n",
    "\n",
    "# Créez ou obtenez l'ID de l'expérience\n",
    "experiment_id = create_or_get_experiment(experiment_name)\n",
    "\n",
    "# Utilisation de la fonction pour enregistrer les caractéristiques d'importance\n",
    "importances = log_feature_importance(experiment_id, train, target_column_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b238b54f-16c1-44ff-94b7-fea43f9d3cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_name = 'TARGET'\n",
    "experiment_name = \"train_and_log_feature_importance\"\n",
    "\n",
    "# Créez ou obtenez l'ID de l'expérience\n",
    "experiment_id = create_or_get_experiment(experiment_name)\n",
    "\n",
    "# Utilisation de la fonction pour enregistrer les caractéristiques d'importance\n",
    "importances = log_feature_importance(experiment_id, train, target_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89292470-2e08-4e98-a772-47db0719a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "# Recherchez les runs de l'expérience spécifiée\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "# Affichez les résultats\n",
    "print(runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd5362-a6ff-49d6-9c7b-3ad20dca97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_xgboost_with_cross_validation_and_log(df, target_column, experiment_name, n_splits=5, random_state=42, eval_metric='logloss'):\n",
    "    \"\"\"\n",
    "    Entraîne un classificateur XGBoost en utilisant la validation croisée, enregistre les résultats avec MLflow,\n",
    "    et retourne les importances moyennes des caractéristiques.\n",
    "\n",
    "    Paramètres :\n",
    "    - df (pd.DataFrame) : Le dataframe d'entrée contenant les caractéristiques et la cible.\n",
    "    - target_column (str) : Le nom de la colonne cible.\n",
    "    - experiment_name (str) : Le nom de l'expérience MLflow.\n",
    "    - n_splits (int) : Le nombre de subdivisions pour la validation croisée KFold.\n",
    "    - random_state (int) : La graine utilisée par le générateur de nombres aléatoires.\n",
    "    - eval_metric (str) : La métrique d'évaluation à utiliser pour l'entraînement du modèle.\n",
    "\n",
    "    Retourne :\n",
    "    - pd.DataFrame : Un dataframe avec deux colonnes : Feature et Importance, triées par importance.\n",
    "    \"\"\"\n",
    "\n",
    "    # Créer ou obtenir l'ID de l'expérience\n",
    "    experiment_id = create_or_get_experiment(experiment_name)\n",
    "\n",
    "    # Séparation des caractéristiques et de la cible\n",
    "    X_train = df.drop(target_column, axis=1)\n",
    "    y_train = df[target_column]\n",
    "\n",
    "    # Configuration de la validation croisée\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    # Initialisation de la liste pour stocker les importances des caractéristiques pour chaque pli\n",
    "    feature_importances_list = []\n",
    "\n",
    "    # Enregistrement d'un nouveau run avec l'ID de l'expérience spécifiée\n",
    "    with mlflow.start_run(experiment_id=experiment_id) as parent_run:\n",
    "        # Boucle sur chaque division de la validation croisée\n",
    "        for fold, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "            # Démarrez un run imbriqué pour chaque pli\n",
    "            with mlflow.start_run(nested=True) as child_run:\n",
    "                # Séparation des données en ensembles d'entraînement et de test\n",
    "                X_train_cv, X_test_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "                y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "                \n",
    "                # Entraînement du modèle\n",
    "                model = xgb.XGBClassifier(random_state=random_state, use_label_encoder=False, eval_metric=eval_metric)\n",
    "                model.fit(X_train_cv, y_train_cv)\n",
    "                \n",
    "                # Stockage des importances des caractéristiques pour le pli actuel\n",
    "                feature_importances_list.append(model.feature_importances_)\n",
    "                \n",
    "                # Enregistrement des paramètres et métriques du modèle pour le pli actuel avec MLflow\n",
    "                mlflow.log_param(\"fold\", fold)\n",
    "                mlflow.sklearn.log_model(model, \"model_fold_{}\".format(fold))\n",
    "                # Vous pourriez également enregistrer des métriques ici avec mlflow.log_metric(...)\n",
    "                \n",
    "                # Les runs imbriqués sont automatiquement fermés à la fin du bloc 'with'\n",
    "    \n",
    "        # Calcul de la moyenne des importances des caractéristiques sur tous les plis\n",
    "        mean_importances = np.mean(feature_importances_list, axis=0)\n",
    "    \n",
    "        # Création d'un DataFrame pour les importances des caractéristiques\n",
    "        importances_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': mean_importances\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        # Enregistrement du DataFrame des importances moyennes en tant que fichier CSV final\n",
    "        final_importances_file = \"mean_feature_importance.csv\"\n",
    "        importances_df.to_csv(final_importances_file, index=False)\n",
    "        \n",
    "        # Enregistrement du fichier CSV final des importances moyennes en tant qu'artefact avec MLflow\n",
    "        mlflow.log_artifact(final_importances_file, \"feature_importance\")\n",
    "        os.remove(final_importances_file)  # Supprimez le fichier CSV après enregistrement\n",
    "    \n",
    "    # Pas besoin d'appeler mlflow.end_run() ici car 'with' s'en occupe\n",
    "    return importances_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16362ca9-6eef-4d91-83af-f9553a9c744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, log_loss  # Importez accuracy_score et log_loss ici\n",
    "import os\n",
    "\n",
    "def train_xgboost_with_cross_validation_and_log(df, target_column, experiment_name, n_splits=5, random_state=42, eval_metric='logloss'):\n",
    "    \"\"\"\n",
    "    Entraîne un classificateur XGBoost en utilisant la validation croisée, enregistre les résultats avec MLflow,\n",
    "    et retourne les importances moyennes des caractéristiques.\n",
    "\n",
    "    Paramètres :\n",
    "    - df (pd.DataFrame) : Le dataframe d'entrée contenant les caractéristiques et la cible.\n",
    "    - target_column (str) : Le nom de la colonne cible.\n",
    "    - experiment_name (str) : Le nom de l'expérience MLflow.\n",
    "    - n_splits (int) : Le nombre de subdivisions pour la validation croisée KFold.\n",
    "    - random_state (int) : La graine utilisée par le générateur de nombres aléatoires.\n",
    "    - eval_metric (str) : La métrique d'évaluation à utiliser pour l'entraînement du modèle.\n",
    "\n",
    "    Retourne :\n",
    "    - pd.DataFrame : Un dataframe avec deux colonnes : Feature et Importance, triées par importance.\n",
    "    \"\"\"\n",
    "\n",
    "    # Créer ou obtenir l'ID de l'expérience\n",
    "    experiment_id = create_or_get_experiment(experiment_name)\n",
    "\n",
    "    # Séparation des caractéristiques et de la cible\n",
    "    X_train = df.drop(target_column, axis=1)\n",
    "    y_train = df[target_column]\n",
    "\n",
    "    # Configuration de la validation croisée\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    # Initialisation de la liste pour stocker les importances des caractéristiques pour chaque pli\n",
    "    feature_importances_list = []\n",
    "\n",
    "    # Enregistrement d'un nouveau run avec l'ID de l'expérience spécifiée\n",
    "    with mlflow.start_run(experiment_id=experiment_id) as parent_run:\n",
    "        # Boucle sur chaque division de la validation croisée\n",
    "        for fold, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "            # Démarrez un run imbriqué pour chaque pli\n",
    "            with mlflow.start_run(nested=True) as child_run:\n",
    "                # Séparation des données en ensembles d'entraînement et de test\n",
    "                X_train_cv, X_test_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "                y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "                \n",
    "                # Entraînement du modèle\n",
    "                model = xgb.XGBClassifier(random_state=random_state, use_label_encoder=False, eval_metric=eval_metric)\n",
    "                model.fit(X_train_cv, y_train_cv)\n",
    "                \n",
    "                # Stockage des importances des caractéristiques pour le pli actuel\n",
    "                feature_importances_list.append(model.feature_importances_)\n",
    "                \n",
    "                # Enregistrement des paramètres et métriques du modèle pour le pli actuel avec MLflow\n",
    "                mlflow.log_params({\n",
    "                    \"fold\": fold,\n",
    "                    \"n_splits\": n_splits,\n",
    "                    \"random_state\": random_state,\n",
    "                    \"eval_metric\": eval_metric\n",
    "                })\n",
    "                \n",
    "                mlflow.log_metrics({\n",
    "                    \"accuracy\": accuracy_score(y_test_cv, model.predict(X_test_cv)),\n",
    "                    \"log_loss\": log_loss(y_test_cv, model.predict_proba(X_test_cv))\n",
    "                })\n",
    "                \n",
    "                mlflow.sklearn.log_model(model, \"model_fold_{}\".format(fold))\n",
    "    \n",
    "        # Calcul de la moyenne des importances des caractéristiques sur tous les plis\n",
    "        mean_importances = np.mean(feature_importances_list, axis=0)\n",
    "    \n",
    "        # Création d'un DataFrame pour les importances des caractéristiques\n",
    "        importances_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': mean_importances\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        # Enregistrement du DataFrame des importances moyennes en tant que fichier CSV final\n",
    "        final_importances_file = \"mean_feature_importance.csv\"\n",
    "        importances_df.to_csv(final_importances_file, index=False)\n",
    "        \n",
    "        # Enregistrement du fichier CSV final des importances moyennes en tant qu'artefact avec MLflow\n",
    "        mlflow.log_artifact(final_importances_file, \"feature_importance\")\n",
    "        os.remove(final_importances_file)  # Supprimez le fichier CSV après enregistrement\n",
    "    \n",
    "    # Pas besoin d'appeler mlflow.end_run() ici car 'with' s'en occupe\n",
    "    return importances_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e006e-1070-4ebd-896c-194c281d1701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Définissez le nom de votre expérience MLflow\n",
    "experiment_name = \"train_and_log_feature_importance_XGBoost_CV\"\n",
    "\n",
    "# 3. Créez ou obtenez l'ID de l'expérience\n",
    "experiment_id = create_or_get_experiment(experiment_name)\n",
    "\n",
    "# 4. Appliquez la fonction à votre DataFrame train avec la colonne cible 'TARGET'\n",
    "importances_cv = train_xgboost_with_cross_validation_and_log(train, 'TARGET', experiment_name)\n",
    "print(importances_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63bbb2d-41a6-46bf-8a22-b62478e405c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance_histogram(importances_cv['Importance'], num_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f0006-3c65-439d-b77c-64a6e973f63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0b2b3-bd8d-493f-befd-7f725a3e1d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "# Recherchez les runs de l'expérience spécifiée\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "# Affichez les résultats\n",
    "print(runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe860e2-272e-4698-8bf5-7e9c47540066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Définissez les run_ids que vous souhaitez examiner\n",
    "run_ids_to_examine = [\"7658b3eb13314f5ebd5b8fff238de6b8\", \"ea383ff9214a44c7a5818d87da9115e3\"]\n",
    "\n",
    "# Obtenez toutes les exécutions de l'expérience\n",
    "all_runs = mlflow.search_runs(experiment_ids=\"408424567618291276\")\n",
    "\n",
    "# Filtrez les exécutions pour les run_ids spécifiés\n",
    "runs_to_examine = all_runs[all_runs['run_id'].isin(run_ids_to_examine)]\n",
    "\n",
    "# Affichez les informations sur les exécutions à examiner\n",
    "print(runs_to_examine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52ad512-5ef8-4cf5-bac0-f85d537e2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "\n",
    "# Remplacez le chemin de l'artefact par celui de l'exécution que vous souhaitez examiner\n",
    "artifact_uri = \"mlflow-artifacts:/408424567618291276/7658b3eb13314f5ebd5b8fff238de6b8/artifacts\"\n",
    "\n",
    "# Obtenez le chemin complet du répertoire de l'artefact\n",
    "artifact_dir = mlflow.get_artifact_uri(artifact_uri)\n",
    "\n",
    "# Affichez la liste des fichiers dans le répertoire de l'artefact\n",
    "for filename in os.listdir(artifact_dir):\n",
    "    if filename.startswith(\"mlflow.log\"):\n",
    "        log_file_path = os.path.join(artifact_dir, filename)\n",
    "        with open(log_file_path, 'r') as log_file:\n",
    "            log_content = log_file.read()\n",
    "            print(f\"Contenu du fichier {filename} :\")\n",
    "            print(log_content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb5d8e-253d-4812-830b-94422dd0275c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
