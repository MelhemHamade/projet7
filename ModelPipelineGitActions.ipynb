{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bf06ca-fdde-4a08-b095-20ff6a443f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fb77625-9ea8-4989-93e1-64877eade84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Après le reset de l'état d'exécution, les imports doivent être refaits\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import resample\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import ipytest\n",
    "import pytest\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70b08e74-5a3b-4db5-9396-fc6df7bb025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def balance_data(df, target_column='TARGET', id_column='SK_ID_CURR', test_size=0.2, taille=None, random_state=42):\n",
    "    \"\"\"\n",
    "    Prépare les données en appliquant le suréchantillonnage SMOTE sur l'ensemble d'entraînement et ajuste la taille finale\n",
    "    de l'ensemble d'entraînement ainsi que de l'ensemble de test. Permet également de réduire la taille de l'ensemble d'entraînement\n",
    "    équilibré à une taille spécifique.\n",
    "\n",
    "    Paramètres:\n",
    "    - df : DataFrame contenant les données.\n",
    "    - target_column : nom de la colonne cible.\n",
    "    - test_size : proportion de l'ensemble de test après réduction.\n",
    "    - train_size : taille souhaitée de l'ensemble d'entraînement après équilibrage (avant réduction).\n",
    "    - taille : taille finale souhaitée de l'ensemble d'entraînement équilibré après réduction.\n",
    "    - random_state : graine pour la reproductibilité.\n",
    "\n",
    "    Retourne:\n",
    "    - X_train_balanced, y_train_balanced : données d'entraînement équilibrées et ajustées à la taille spécifiée.\n",
    "    - X_test_reduced, y_test_reduced : données de test réduites selon test_size.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Exclure la colonne cible et éventuellement la colonne d'identifiant\n",
    "    if id_column:\n",
    "        X = df.drop([target_column, id_column], axis=1)\n",
    "    else:\n",
    "        X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Fractionnement initial pour créer un ensemble de test non touché\n",
    "    X_train_initial, X_test_initial, y_train_initial, y_test_initial = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Application de SMOTE sur l'ensemble d'entraînement\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_initial, y_train_initial)\n",
    "    \n",
    "    # Réduction de l'ensemble d'entraînement équilibré à la taille spécifiée\n",
    "    if taille and len(X_train_balanced) > taille:\n",
    "        X_train_balanced, y_train_balanced = resample(X_train_balanced, y_train_balanced,\n",
    "                                                      replace=False, n_samples=taille,\n",
    "                                                      random_state=random_state)\n",
    "\n",
    "    # Réduction de l'ensemble de test si nécessaire\n",
    "    if test_size < 1.0:\n",
    "        X_test_reduced, _, y_test_reduced, _ = train_test_split(X_test_initial, y_test_initial, test_size=test_size, random_state=random_state)\n",
    "    else:\n",
    "        X_test_reduced, y_test_reduced = X_test_initial, y_test_initial\n",
    "\n",
    "    return X_train_balanced, y_train_balanced, X_test_reduced, y_test_reduced\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34cfb1cd-7bda-4c15-991b-a7993b9e41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_cost(y_true, y_pred, cost_fn, cost_fp):\n",
    "    \"\"\"\n",
    "    Calcule le coût métier basé sur les faux négatifs et les faux positifs.\n",
    "    \"\"\"\n",
    "    fn = sum((y_pred == 0) & (y_true == 1))\n",
    "    fp = sum((y_pred == 1) & (y_true == 0))\n",
    "    return fn * cost_fn + fp * cost_fp\n",
    "\n",
    "def business_score_metric(y_true, y_pred, cost_fn, cost_fp):\n",
    "    \"\"\"\n",
    "    Métrique personnalisée qui calcule le business score pour la validation croisée.\n",
    "    \"\"\"\n",
    "    fn = sum((y_pred == 0) & (y_true == 1))\n",
    "    fp = sum((y_pred == 1) & (y_true == 0))\n",
    "    return - (fn * cost_fn + fp * cost_fp)  # Négatif car GridSearchCV cherche à minimiser la métrique\n",
    "\n",
    "def find_optimal_threshold(y_test, y_scores, cost_fn, cost_fp):\n",
    "    \"\"\"\n",
    "    Trouve le seuil optimal pour la classification.\n",
    "\n",
    "    Paramètres :\n",
    "    - y_test : valeurs réelles\n",
    "    - y_scores : scores de probabilité prédits par le modèle\n",
    "    - cost_fn : coût d'un faux négatif\n",
    "    - cost_fp : coût d'un faux positif\n",
    "\n",
    "    Retourne :\n",
    "    - seuil optimal pour la classification\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    costs = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_scores >= threshold).astype(int)\n",
    "        cost = business_cost(y_test, y_pred, cost_fn, cost_fp)\n",
    "        costs.append(cost)\n",
    "    \n",
    "    # Trouver le seuil avec le coût le plus bas\n",
    "    optimal_idx = np.argmin(costs)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    return optimal_threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71afd76f-8f3a-475d-85e2-46110c117cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPipeline:\n",
    "    def __init__(self, models, param_grids, scoring='roc_auc', cost_fn=10, cost_fp=1, test_size=0.2, taille=None, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialisation du pipeline.\n",
    "\n",
    "        :param models: Liste des modèles de machine learning à évaluer.\n",
    "        :param param_grids: Liste des dictionnaires contenant les grilles d'hyperparamètres correspondant à chaque modèle.\n",
    "        :param cost_fn: Coût associé à un faux négatif.\n",
    "        :param cost_fp: Coût associé à un faux positif.\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.param_grids = param_grids\n",
    "        self.cost_fn = cost_fn\n",
    "        self.cost_fp = cost_fp\n",
    "        self.test_size=test_size\n",
    "        self.taille=taille\n",
    "        self.random_state=random_state\n",
    "        self.results = []  # Pour stocker les résultats de chaque modèle\n",
    "        self.scoring = scoring\n",
    "        \n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    def optimize_hyperparameters(self, model, param_grid, scoring, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Fonction pour optimiser les hyperparamètres d'un modèle donné.\n",
    "\n",
    "        :param model: Modèle à optimiser.\n",
    "        :param param_grid: Grille des hyperparamètres à tester.\n",
    "        :param X_train: Données d'entraînement.\n",
    "        :param y_train: Étiquettes d'entraînement.\n",
    "        :return: Meilleur modèle après optimisation.\n",
    "        \"\"\"\n",
    "        # Si une grille d'hyperparamètres est fournie, utilisez GridSearchCV pour trouver le meilleur ensemble d'hyperparamètres\n",
    "        if param_grid:\n",
    "            grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, n_jobs=-1, cv=5, verbose=1)\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            return grid_search.best_estimator_, grid_search.best_params_\n",
    "        else:\n",
    "            # Sinon, entraînez simplement le modèle avec les hyperparamètres par défaut\n",
    "            model.fit(X_train, y_train)\n",
    "            return model, model.get_params()\n",
    "\n",
    "    \n",
    "    def train_and_evaluate(self, model, X_train, y_train, X_test, y_test, param_grid):\n",
    "        \"\"\"\n",
    "        Fonction pour entraîner et évaluer un modèle.\n",
    "\n",
    "        :param model: Modèle à évaluer.\n",
    "        :param X_train: Données d'entraînement.\n",
    "        :param y_train: Étiquettes d'entraînement.\n",
    "        :param X_test: Données de test.\n",
    "        :param y_test: Étiquettes de test.\n",
    "        :param param_grid: Grille d'hyperparamètres pour l'optimisation.\n",
    "        \"\"\"\n",
    "        start_time = time.time()  # Début du chronométrage de l'entraînement et de l'évaluation\n",
    "        with mlflow.start_run():\n",
    "            \n",
    "            best_model, best_params = self.optimize_hyperparameters(model, param_grid, self.scoring, X_train, y_train)\n",
    "            for param, value in best_params.items():\n",
    "                mlflow.log_param(param, value)\n",
    "            # Évaluation du modèle et calcul des métriques\n",
    "            start_time_prediction = time.time()  # Début du chronométrage de prédiction\n",
    "            y_scores = best_model.predict_proba(X_test)[:, 1]\n",
    "            prediction_time = time.time() - start_time_prediction  # Calcul du temps de prédiction\n",
    "            optimal_threshold = find_optimal_threshold(y_test, y_scores, self.cost_fn, self.cost_fp)\n",
    "            y_pred_optimal = (y_scores >= optimal_threshold).astype(int)\n",
    "            business_score = business_cost(y_test, y_pred_optimal, self.cost_fn, self.cost_fp)\n",
    "            accuracy = accuracy_score(y_test, y_pred_optimal)\n",
    "            auc_score = roc_auc_score(y_test, y_scores)\n",
    "            # Enregistrement des métriques dans MLflow\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"auc\", auc_score)\n",
    "            mlflow.log_metric(\"business_score\", business_score)\n",
    "            mlflow.log_metric(\"optimal_threshold\", optimal_threshold)\n",
    "            # Enregistrement du modèle dans MLflow\n",
    "            mlflow.sklearn.log_model(best_model, \"best_model\")\n",
    "        execution_time = time.time() - start_time  # Calcul du temps d'exécution total\n",
    "        \n",
    "        # Ajout des résultats dans un DataFrame\n",
    "        results = {\n",
    "            'Model': type(model).__name__,\n",
    "            'Accuracy': accuracy,\n",
    "            'AUC': auc_score,\n",
    "            'Business Score': business_score,\n",
    "            'Optimal Threshold': optimal_threshold,\n",
    "            'Execution Time': execution_time,\n",
    "            'Prediction Time': prediction_time,\n",
    "            'Best Model': best_model  # Stockage du modèle pour une utilisation ultérieure\n",
    "        }\n",
    "        self.results.append(results)\n",
    "\n",
    "    def run_pipeline(self, X, target_column, id_column):\n",
    "        \"\"\"\n",
    "        Exécute le pipeline pour tous les modèles fournis.\n",
    "\n",
    "        :param X_train: Données d'entraînement.\n",
    "        :param y_train: Étiquettes d'entraînement.\n",
    "            :param X_test: Données de test.\n",
    "        :param y_test: Étiquettes de test.\n",
    "        \"\"\"\n",
    "        # Séparation, équilibrage et réduction\n",
    "        X_train, y_train, X_test, y_test = balance_data(X, target_column, id_column, test_size=self.test_size, taille=self.taille, random_state=self.random_state)\n",
    "        \n",
    "        for model, param_grid in zip(self.models, self.param_grids):\n",
    "            print(f\"Entraînement et évaluation du modèle : {type(model).__name__}\")\n",
    "            self.train_and_evaluate(model, X_train, y_train, X_test, y_test, param_grid)\n",
    "        \n",
    "        # Après l'exécution de tous les modèles, convertir les résultats en DataFrame pour une analyse facile\n",
    "        results_df = pd.DataFrame(self.results)\n",
    "        return results_df\n",
    "    \n",
    "def get_best_model_with_threshold(results_df, metric):\n",
    "    \"\"\"\n",
    "    Sélectionne le meilleur modèle basé sur une métrique spécifique et retourne le seuil optimal.\n",
    "\n",
    "    :param results_df: DataFrame contenant les résultats de l'évaluation de chaque modèle.\n",
    "    :param metric: La métrique sur laquelle baser la sélection du meilleur modèle.\n",
    "    :return: Tuple contenant le meilleur modèle et son seuil optimal.\n",
    "    \"\"\"\n",
    "    best_row = results_df.loc[results_df[metric].idxmin()]\n",
    "    best_model = best_row['Best Model']\n",
    "    optimal_threshold = best_row['Optimal Threshold']\n",
    "    return best_model, optimal_threshold\n",
    "\n",
    "\n",
    "random_state=42\n",
    "\n",
    "models = [\n",
    "    xgb.XGBClassifier(random_state=random_state, n_jobs=-1),\n",
    "    LogisticRegression(random_state=random_state, solver='saga', max_iter=5000, n_jobs=-1),\n",
    "]\n",
    "\n",
    "# Grille d'hyperparamètres simple pour XGBClassifier\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Grille d'hyperparamètres simple pour LogisticRegression\n",
    "lr_param_grid = {\n",
    "    'C': [0.1, 1.0],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "param_grids = [\n",
    "  xgb_param_grid, lr_param_grid  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77ffe27b-784e-4c7d-b50f-66a8e990e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f80746-da20-44aa-a333-a34e596c61e1",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9009f9f3-037e-4d3b-bb32-aae719faed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture(scope=\"function\")\n",
    "def test_pipeline_execution():\n",
    "    \n",
    "    pipeline = ModelPipeline(models, param_grids, scoring='roc_auc', cost_fn=10, cost_fp=1, test_size=0.2, taille=5000, random_state=42)\n",
    "    results_df = pipeline.run_pipeline(train, 'TARGET', 'SK_ID_CURR')\n",
    "    return results_df\n",
    "    \n",
    "\n",
    "def test_pipeline_results(test_pipeline_execution):\n",
    "    results_df = test_pipeline_execution\n",
    "    assert not results_df.empty, \"Le DataFrame des résultats est vide\"\n",
    "\n",
    "    # Vérifier les plages pour Accuracy, AUC et Optimal Threshold\n",
    "    assert all((0 <= results_df['Accuracy']) & (results_df['Accuracy'] <= 1)), \"La métrique Accuracy est en dehors de la plage [0, 1]\"\n",
    "    assert all((0 <= results_df['AUC']) & (results_df['AUC'] <= 1)), \"La métrique AUC est en dehors de la plage [0, 1]\"\n",
    "    assert all((0 <= results_df['Optimal Threshold']) & (results_df['Optimal Threshold'] <= 1)), \"La métrique Optimal Threshold est en dehors de la plage [0, 1]\"\n",
    "\n",
    "    # Vérifier si le nombre d'entrées dans results_df correspond au nombre d'observations dans X_train_test\n",
    "    assert len(results_df) == len(models), \"Incohérence dans le nombre d'entrées entre results_df et X_train_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13eff09c-bd0e-4659-9075-5684cb03d8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.11.4, pytest-8.0.2, pluggy-1.4.0\n",
      "rootdir: C:\\Users\\Melhem\\Desktop\\FormationOpenClassRooms\\projet7bis\n",
      "plugins: anyio-4.3.0\n",
      "collected 1 item\n",
      "\n",
      "t_957a51b545894bf5acf959616926d1d0.py \u001b[32m.\u001b[0m\u001b[32m                                                      [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m======================================== warnings summary =========================================\u001b[0m\n",
      "t_957a51b545894bf5acf959616926d1d0.py::test_pipeline_results\n",
      "t_957a51b545894bf5acf959616926d1d0.py::test_pipeline_results\n",
      "  C:\\Program Files\\Python311\\Lib\\importlib\\__init__.py:126: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================================= \u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m2 warnings\u001b[0m\u001b[33m in 48.46s\u001b[0m\u001b[33m ==================================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pour exécuter les tests dans une cellule Jupyter, utilisez ipytest pour une intégration facile\n",
    "\n",
    "ipytest.autoconfig()\n",
    "\n",
    "# Exécutez les tests\n",
    "ipytest.run('-v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a77c1-2f2e-4d9f-b86d-4bff831569c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
